#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass report
\use_default_options true
\begin_modules
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 3cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 2
\paperpagestyle headings
\bullet 0 1 1 -1
\bullet 1 0 31 -1
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
\begin_inset Graphics
	filename figures/logo_LOA.pdf
	width 12cm

\end_inset


\begin_inset Newline newline
\end_inset

Technical report on project Sketch II
\end_layout

\begin_layout Date
sept.
 2013
\end_layout

\begin_layout Author
M.
 Moussallam
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section*
Context of this study
\end_layout

\begin_layout Standard
This document summarizes the technical details of the framework built as
 part of the Sketch 2 (a.k.a.
 Ghost Sounds) project.
 More precisely, it is part of the work-packages (WP) 1.1 and 1.2.
 For the sake of readability and clarity, the amount of technical details
 is limited to what is necessary for apprehending the basics concepts.
\end_layout

\begin_layout Subsection*
Goals and Tasks
\end_layout

\begin_layout Standard
The first phase of the project aims at building a 
\begin_inset Quotes eld
\end_inset

sketchifying
\begin_inset Quotes erd
\end_inset

 process tailored for priming experiments.
 The deliverable should take the form of a reusable piece of software that
 is able to build a 
\begin_inset Quotes eld
\end_inset

sketch
\begin_inset Quotes erd
\end_inset

 from any given audio scene.
\end_layout

\begin_layout Itemize
WP 1.1 consists in the definition and characterization of audio 
\begin_inset Quotes eld
\end_inset

sketches
\begin_inset Quotes erd
\end_inset

 via a signal and/or a statistical model.
 We are interested in finding not only the main features of sounds, but
 more importantly their peculiar ones.
 In order to do so, one must first decide of a feature space on which to
 project the signals, and second, distinguish which of these feature (or
 sets of features) are discriminative.
\end_layout

\begin_layout Itemize
WP 1.2 addresses the problem of validation via an automatic recognition task.
 The recognition is performed by means of acoustic fingerprints comparison.
 This research field being a mature one, the objective can not be to compete
 with existing industrial solution.
 Nonetheless, those should serve as a basis for benchmarking any proposed
 fingerprint scheme.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
The goal of this task is to build a sketches model for a specific class
 of sounds under study (for instance in the statistics of selected features,
 and maybe more importantly in their pairwise correlation).
 From this model and a given sound, we can now investigate how can we build
 a sound “fingerprint” that is as compact as possible, and that uniquely
 identifies this sound.
 From an information theory standpoint, the selected features would not
 be the ones that simply best describe the sound, but the ones that best
 describe the sound knowing the sound model, in other words the features
 that distinguish the individual from the group.
 To come back to the original visual metaphor, we do not desire a realistic
 “sketch” anymore, but a voluntarily distorted “caricature” that emphasizes
 the specifics of the original object and forgets about features that may
 be large in absolute terms but in standard proportion in the average model
 (when we want to describe a person’s face, we do not start by saying “he
 has a mouth, a nose and two eyes”, as these are important but obvious features,
 we only focus on small specific details “he has a square chin and large
 ears”).
 Fundamentally speaking, this can be seen as a generalized notion of sound
 timbre, not as an instrinsic quality of the sound, but as a notion relative
 to the other sounds (the original notion of timbre is the set of characteristic
s that allow us to distinguish two sounds that have the same pitch, duration,
 and loudness).
 
\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
Object and concept definitions
\end_layout

\begin_layout Description
Sounds Any digital audio signal, as the discrete sampling of an acoustic
 wave.
 For simplicity's sake, the spatial properties may be discarded and only
 mono-channel signals may be considered.
 Among properties one may find the sampling frequency and the length or
 duration.
\end_layout

\begin_layout Description
Track An audio track is a long sound (in the order of 30 seconds to several
 minutes) that may for instance be a musical interpretation.
 Among properties one may find the title, artist name, duration, musical
 genre or speaker id.
\end_layout

\begin_layout Description
Scene An audio scene is a short sound, e.g.
 a single sentence uttered by a speaker or an environmental sound such as
 a car passing, or a precise part in an audio track (e.g.
 chorus or verse)
\end_layout

\begin_layout Description
DataSet A collection of sounds (or tracks).
\end_layout

\begin_layout Description
Sketch A simplified version of an audio scene.
 By simplify we mean any way to approximate the original object via dimensionali
ty reduction.
 We assume the reader is aware of the Sketch 1 paper 
\begin_inset CommandInset citation
LatexCommand cite
key "Suied2013"

\end_inset

.
\end_layout

\begin_layout Description
Fingerprint A hash version of a sketch, that can be stored and retrieved
 efficiently from a fingerprint database
\end_layout

\begin_layout Description
Fingerprint
\begin_inset space ~
\end_inset

Database A hashtable of key/value pairs built from the acoustic fingerprints
 of an entire DataSet.
 The storing and retrieval is facilitated either by an efficient hash indexing
 or a tree structure.
 
\end_layout

\begin_layout Subsection*
Participants
\end_layout

\begin_layout Standard
The project officially started in march 2013.
 WP 1.1 and 1.2 are conducted by Pr.
 L.
 Daudet and Dr.
 M.
 Moussallam.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
Building Sound Sketches
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $x\in\mathbb{R}^{N}$
\end_inset

 be a digital signal of finite length 
\begin_inset Formula $N$
\end_inset

.
 A sketch 
\begin_inset Formula $sketch(x)$
\end_inset

 of 
\begin_inset Formula $x$
\end_inset

 is a simplified version of 
\begin_inset Formula $x$
\end_inset

 that can be inverted.
 By simplification, we usually mean that 
\begin_inset Formula $sketch(x)$
\end_inset

 lives in a manifold of a lower dimension than 
\begin_inset Formula $N$
\end_inset

 or that it has a sparse expression in a higher dimensional space.
 Before computing a sketch, one usually uses a representation 
\begin_inset Formula $Rep(x)$
\end_inset

 to transpose the signal in such 
\emph on
feature space
\emph default
.
 A popular choice of representations are the ones that define a mapping
 onto a Time-Frequency plane.
 
\end_layout

\begin_layout Standard
A sketch is thus typically the result of a processing of a representation
 of 
\begin_inset Formula $x$
\end_inset

 (for instance, the selection of the most prominent elements in the feature
 space).
 In a final stage, sketches will be used to build fingerprints 
\begin_inset Formula $fgpt(x)$
\end_inset

.
 Those are designed so as to capture the specificity of a sketch and most
 importantly allow fast and robust comparisons between the sketches of two
 sounds.
\end_layout

\begin_layout Standard
The usual process will thus look like:
\begin_inset Formula 
\[
x\rightarrow Rep(x)\rightarrow sketch(x)\rightarrow fgpt(x)
\]

\end_inset

 Quite naturally, let us now introduce the typical representations, then
 the sketches and finally the fingerprints.
\end_layout

\begin_layout Section
Representations
\end_layout

\begin_layout Subsection
Spectrograms
\end_layout

\begin_layout Standard
The standard Short-Term Fourier Transform (STFT) is a lapped transform using
 a standard Fourier transform on overlapping (often windowed) segments.
 Let 
\begin_inset Formula $w$
\end_inset

 be a window of size 
\begin_inset Formula $L$
\end_inset

, the STFT of 
\begin_inset Formula $x$
\end_inset

 is indexed in time by the frame (i.e.
 segment) index 
\begin_inset Formula $p$
\end_inset

 (
\begin_inset Formula $0\leq p\leq P-1)$
\end_inset

 and in frequency by 
\begin_inset Formula $k$
\end_inset

 (
\begin_inset Formula $0\leq k\leq k-1)$
\end_inset

:
\begin_inset Formula 
\[
R_{STFT}(x)[p,k]=\frac{1}{\gamma}\sum_{n=0}^{L-1}w[n]\cdot x[n-n_{p}]\cdot\exp\left(-2j\pi k\frac{n}{L}\right)
\]

\end_inset

where 
\begin_inset Formula $\gamma$
\end_inset

 is a normalization factor and 
\begin_inset Formula $n_{p}$
\end_inset

 is the step between two consecutively analyzed frames.
 The STFT is a complex representation, in order to visualize and /or manipulate,
 one usually consider the logarithm of its magnitude.
 This is referred to as the spectrogram of 
\begin_inset Formula $x$
\end_inset

:
\begin_inset Formula 
\[
R_{Spectrogram}(x)=\log\left(|R_{STFT}(x)|\right)
\]

\end_inset

properties:
\end_layout

\begin_layout Enumerate
The STFT has a fixed and linear both in time and frequency resolution.
\end_layout

\begin_layout Enumerate
The STFT is perfectly invertible, provided the Constant Overlap-Add condition
 is fulfilled, meaning basically that the windows are smooth enough and
 must sum to one.
 In practice this is obtained by choosing a Hann window for 
\begin_inset Formula $w$
\end_inset

 and ensuring an overlap of 50% or more (i.e.
 
\begin_inset Formula $n_{p}\leq\frac{L}{2}$
\end_inset

).
\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/exemple_stft.pdf
	width 8cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Spectrogram (Logarithm of the magnitude of the Short Time Fourier Transform)
 of an audio excerpt.
 Darker regions are more energetic.
 The Time/Frequency resolution obeys a compromise that is controlled by
 the analysis window size.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Subsection
Constant Q transform
\end_layout

\begin_layout Standard
The Constant Q transform (CQT) is a TF representation in which the frequency
 scale is logarithmic.
 Unlike the STFT, the CQT cannot be achieved by using a single windowing.
 Instead a filter bank must be defined, whose filters length 
\begin_inset Formula $L_{k}$
\end_inset

 are dyadic in order to ensure that the Q factor:
\begin_inset Formula 
\[
Q=\frac{f_{r}}{\Delta f}
\]

\end_inset

with 
\begin_inset Formula $f_{r}$
\end_inset

 being the resonant frequency of the filter and 
\begin_inset Formula $\Delta f$
\end_inset

 the half-bandwidth, remains constant.
 The representation is then defined by:
\begin_inset Formula 
\[
R_{CQT}(x)[k]=\frac{1}{L_{k}}\sum_{n=0}^{L_{k}-1}w_{k}[n]\cdot x[n]\exp\left(-2j\pi n\frac{f_{k}}{f_{s}}\right)
\]

\end_inset

where 
\begin_inset Formula $w_{k}$
\end_inset

 is a frequency-dependent kernel window, 
\begin_inset Formula $f_{k}$
\end_inset

 are the center frequencies of the filters and 
\begin_inset Formula $f_{s}$
\end_inset

 is the sampling rate.
\end_layout

\begin_layout Standard
In many musical applications (and fingerprinting is no exception 
\begin_inset CommandInset citation
LatexCommand cite
key "gretsi"

\end_inset

), the frequency scale chosen matches the MIDI musical scale.
 A 440Hz A4 is defined by index 69 of the MIDI scale:
\begin_inset Formula 
\[
midi(f)=69+12\log_{2}\left(\frac{f}{440}\right)
\]

\end_inset


\end_layout

\begin_layout Subsection
Cochleogram
\end_layout

\begin_layout Standard
Also known as auditory spectrum 
\begin_inset CommandInset citation
LatexCommand cite
key "Yang1992,Wang1995"

\end_inset

, the cochleogram is the magnitude of a log-frequency transform similar
 in spirit to the CQT but using real auditory response filters (from the
 cochlea) as the filter banks.
 Those filters are called 
\emph on
Gammatone auditory filters.
 
\emph default
Their main difference with Constant Q filters and Gabor filters (used in
 STFT) is their non-symmetricalness.
 A gammatone filter centered at frequency 
\begin_inset Formula $f_{c}$
\end_inset

 writes in the time domain:
\begin_inset Formula 
\[
gt(t)=at^{(n-1)}\exp\left(-2\pi bt\right)\cos\left(2\pi f_{c}t+\phi\right)
\]

\end_inset

it is basically a cosine windowed by a non-symmetric windows that is the
 product of a power term and a decreasing exponential.
 the parameters mainly controls the filter duration and therefore the bandwidth.
 These characteristic values can be experimentally learned.
 For the sake of clarity we will use the same set of filter coefficients
 used in existing software (i.e.
 the NSL toolbox).
\end_layout

\begin_layout Standard
Cochleograms are the visualization of the gammatone filters output magnitudes.
 They somewhat emphasize the TF regions that are perceptually stimulated.
 A nice introduction to cochleograms can be found in 
\begin_inset CommandInset citation
LatexCommand cite
key "Viberg2004"

\end_inset

 and further details in 
\begin_inset CommandInset citation
LatexCommand cite
key "Yang1992,Wang1995"

\end_inset

.
 The cochleogram computation is a direct transcription of the one performed
 in the NSL Toolbox
\begin_inset Foot
status open

\begin_layout Plain Layout
Doc and download at http://www.isr.umd.edu/Labs/NSL/Downloads.html
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/exemple_cochleogram.pdf
	width 8cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Cochleogram (absolute value) of an audio excerpt.
 Darker regions are more energetic.
 The frequency scale is logarithmic and the Time/Frequency resolution compromise
 is frequency dependent.
 The filter bank models the first step of the human auditory system.
 Low and high frequencies are typically lowered while frequencies in the
 500-2000Hz range are emphasized.
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Corticograms
\end_layout

\begin_layout Standard
Even deeper into the human auditory system is the primary area of the cortex.
 The output of the cochlear system is processed by neuron layers whose action
 can be approximately modeled by 2D wavelet transforms.
 The 
\emph on
corticogram
\emph default
 is a 4D representation indexed as a scale/rate/time/frequency matrix.
 As such it can be figured as a matrix of Time-Frequency representations.
 
\end_layout

\begin_layout Standard
Evidences suggest that groups of neurons in the primary area of the auditory
 cortex (A1) perform a processing that is tuned to a certain scale and rate
 and looks like a filtering with some additional non-linear behaviors that
 can be neglected in a first approximation.
 A different way of understanding the Corticogram of a sound, is to view
 it as a kind of double Fourier analysis (more precisely a double wavelet
 transform) where the cochlea performs the first stage and the cortex the
 second one.
\end_layout

\begin_layout Standard
Interested reader should dig into the paper 
\begin_inset CommandInset citation
LatexCommand cite
key "Wang1995"

\end_inset

 for more details.
 Again, the implementation that has been done in this work mimics the one
 of the NSL toolbox.
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/exemple_corticogram.pdf
	width 16cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Corticogram visualization as a matrix of time-frequency representations.
 Each of the plots can be understood as a cochleogram that has been filtered
 in 2D.
 Filter characteristics are controlled by the scale and rate parameters.
 In the Top-Left corner, one can see a TF plot that is very smooth both
 in time and frequency.
 Increasing the scale parameters lead to lower plots.
 The Bottom-Left corner plot depicts energy with a good frequency resolution
 but is highly smooth in time.
 At the opposite, the Top-Right corner plots is smooth in frequency but
 sharp in time.
 The Bottom-Right corner somehow presents the high frequencies of the 
\begin_inset Quotes eld
\end_inset

wavelet transform
\begin_inset Quotes erd
\end_inset

 and looks much more like the Cochleogram
\begin_inset CommandInset label
LatexCommand label
name "fig:Corticogram-visualization"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Questions raised by the corticogram
\end_layout

\begin_layout Standard
The corticogram is a 4-D object and thus not intuitively manipulated.
 As Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Corticogram-visualization"

\end_inset

 shows, it can be visualized as a grid where each element corresponds to
 a possible 2D local ripple filtering.
 The ripple parameters are precisely the rate and scale parameters, also
 labeled ripple frequency and phase.
\end_layout

\begin_layout Standard
Which of these multiple representation preserve the discriminative information
 the best? It is not clear how this multi-staged analysis enables the mammal
 brain to robustly process sounds.
 Intuition suggests a specialization of brain areas to spectral, temporal
 and hybrid sort of processing.
 This intuition is backed by our understanding of how sounds, and particularly
 music is organized in melodies (harmonic content) and rhythm.
 
\end_layout

\begin_layout Subsection
Sparse Multi-Resolution Approximation
\end_layout

\begin_layout Standard
Corticograms can be considered as a multi-resolution view of cochleograms.
 A different class of multi-resolution representations can be built using
 the theory of frames 
\begin_inset CommandInset citation
LatexCommand cite
key "Mallat2009"

\end_inset

 and more generally using a union of analysis 
\emph on
dictionaries
\emph default
.
 A dictionary 
\begin_inset Formula $\Phi$
\end_inset

 is any family of 
\begin_inset Formula $M$
\end_inset

 vectors 
\begin_inset Formula $\phi_{i}$
\end_inset

.
 One is typically interested in solving a synthesis problem of finding the
 coefficients 
\begin_inset Formula $\alpha_{i}$
\end_inset

 such that:
\begin_inset Formula 
\begin{equation}
x=\sum_{i=0}^{M-1}\alpha_{i}\phi_{i}\label{eq:synthesis-dico}
\end{equation}

\end_inset

which poses no problem when the dictionary is an orthonormal basis.
 But in order to achieve multi-resolution, one might built a dictionary
 as a union of orthonormal basis (for instance, STFT of different window
 length or MDCT).
 However, in such 
\emph on
overcomplete
\emph default
 dictionaries, their might not be a unique set of coefficient satisfying
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:synthesis-dico"

\end_inset

.
 The problem of 
\emph on
sparse representation
\emph default
 therefore consists in finding, among all possible solutions the one that
 minimizes a 
\emph on
sparsity cost function, 
\emph default
usually the 
\begin_inset Formula $\ell_{0}$
\end_inset

-pseudo norm.
 The sparse 
\emph on
representation
\emph default
 problem thus writes:
\begin_inset Formula 
\begin{equation}
\min_{\alpha}\|\mathbf{\alpha}\|_{0}\mbox{ such that }x=\sum_{i=0}^{M-1}\alpha_{i}\phi_{i}\label{eq:l0_prob}
\end{equation}

\end_inset

and its sparse 
\emph on
approximation
\emph default
 counterpart:
\begin_inset Formula 
\begin{equation}
\min_{\alpha}\|\mathbf{\alpha}\|_{0}\mbox{ such that }\|x-\sum_{i=0}^{M-1}\alpha_{i}\phi_{i}\|^{2}\leq\epsilon\label{eq:l0_approx}
\end{equation}

\end_inset

which are (most of the time) NP-hard to solve.
 Potentially sub-optimal solutions are found using selection mechanism such
 as thresholding or greedy algorithms.
 There is a huge literature on these methods (for instance 
\begin_inset CommandInset citation
LatexCommand cite
key "Mallat_TSP1993,Chen98atomicdecomposition,Donoho2006a"

\end_inset

), but for practical purposes, one can look into the PyMP tutorials
\begin_inset Foot
status open

\begin_layout Plain Layout
http://manuel.moussallam.net/PyMP/
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/sketchified_panzani_XMDCTSparseSketch.pdf
	width 8cm

\end_inset


\begin_inset Graphics
	filename figures/Example_3atoms_mdct.pdf
	width 8cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Left: Spectrogram of the resynthesis of a sparse approximation in a union
 of 3 MDCT basis.
 Red regions are more energetic.
 Right: the 3 types of basic components (or atoms) that are used.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Remark on sparse approximation
\end_layout

\begin_layout Standard
As one can notice, here the representation is in itself a 
\emph on
sketchification
\emph default
 process.
 The multi-resolution representation space that is chosen and most importantly
 the sparsity prior that is enforced on the representation amounts to a
 
\emph on
feature selection
\emph default
 process in a high-dimensional space.
 Since they are no exact representation but already a simplification, sparse
 approximations can be viewed as sketches of sound.
\end_layout

\begin_layout Section
Sketches
\end_layout

\begin_layout Standard
A sketch is a simplified version of an audio signal.
 Usually a sketch is obtained by reducing the dimensionality in a transform
 domain or equivalently in a 
\emph on
feature space.
 
\emph default
The selection of a subset of these features constitutes a sketch.
 Equivalently, a sketch is any sparse combination of atoms in a given 
\emph on
dictionary
\emph default
.
\end_layout

\begin_layout Subsection
Time-Frequency Peaks
\end_layout

\begin_layout Standard
Peak-picking consists in finding the local extreme values.
 At the core of the fingerprinting scheme proposed by Wang in the Shazam
 method, peaks are selected in square regions of the Time frequency plane
 in order to maximize their mutual information.
 It is indeed easy to show that (except for completely noisy processes)
 time-frequency features of audio signals are strongly correlated locally.
 In this work, peak picking has been implemented on Spectrograms, Cochleograms
 and Corticograms.
\end_layout

\begin_layout Subsection
Sparse approximation 
\end_layout

\begin_layout Standard
As we have already seen in section 1.1.5, sparse approximations can be considered
 as sketches.
 The feature selection is the result of algorithms that are tailored for
 the problem 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:l0_approx"

\end_inset

.
 Two main classes of methods have been implemented so far:
\end_layout

\begin_layout Itemize
Greedy algorithms such as Matching Pursuit.
 These algorithm alternate the selection of a component of the representation
 and a reprojection of the residual.
\end_layout

\begin_layout Itemize
Thresholding algorithm, (Iterative Hard Thresholding, Soft etc..).
 These algorithm select multiple features at each iteration and perform
 a thresholding of the non-selected features, followed by a shrinkage procedure.
\end_layout

\begin_layout Subsection
SineWave Speech
\end_layout

\begin_layout Standard
The sine wave speech synthesis is obtained by tracking formants (resonant
 frequencies of the vocal track) along time.
 The simplified sound surprisingly reminds of the original.
 This representation is obtained by first finding the formants in the spectrogra
m, then using a Viterbi algorithm for the tracking.
\end_layout

\begin_layout Standard
There is a very good introductory article on sinewave speech on 
\begin_inset CommandInset href
LatexCommand href
name "scholarpedia"
target "http://www.scholarpedia.org/article/Sine-wave_speech"

\end_inset

.
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/sketchified_panzani_SWSSketch.pdf
	width 8cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Spectrogram of a SineWave Speech reconstruction of an audio excerpt.
 red regions are more energetic.
 The signal is sliced in evenly spaced time frames.
 In each frame, the formants (resonant frequencies of the vocal track) are
 searched for.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Audio Feature vector
\end_layout

\begin_layout Standard
Literature on audio feature computation is proficient.
 Any feature extraction process can be used, (MFCC, chroma, LPC, etc..) any
 feature combination can be thought of as a sketch of the signal.
 Most of the time, the audio features are analysis-oriented and cannot be
 inverted.
 This is for instance the case with Zero-Crossing Rate or spectral moments.
 
\end_layout

\begin_layout Standard
The synthesis can then be obtained by concatenating sounds from a database
 based on nearest neighbors search in the feature space.
 This is for example used in 
\begin_inset CommandInset citation
LatexCommand cite
key "Muller2005"

\end_inset

.
\end_layout

\begin_layout Section
Fingerprints
\end_layout

\begin_layout Standard
There is quite a large literature on fingerprinting problems and systems.
 The PhD thesis of J.
 Pinquier 
\begin_inset CommandInset citation
LatexCommand cite
key "Pinquier_phD"

\end_inset

 is a good place to start but a bit old.
 An updated information can be found in S.
 Fenet PhD Thesis that is to be defended quite soon.
 Among very classical fingerprint systems is the one proposed by Haitsma
 
\begin_inset CommandInset citation
LatexCommand cite
key "Haitsma"

\end_inset

, the very famous Shazam system 
\begin_inset CommandInset citation
LatexCommand cite
key "Wang_com"

\end_inset

 and the Phillips one 
\begin_inset CommandInset citation
LatexCommand cite
key "Philips_DCT,Philips_noise"

\end_inset

.
 
\end_layout

\begin_layout Subsection
Families of fingerprints
\end_layout

\begin_layout Standard
There are two families of fingerprinting systems:
\end_layout

\begin_layout Enumerate
The ones that build a single identifier out of an audio signal (e.g.
 a feature vector) and compare audio signals through their computed identifiers.
 (see the review of all such - prior to 2002 - techniques in 
\begin_inset CommandInset citation
LatexCommand cite
key "Cano_review"

\end_inset

)
\end_layout

\begin_layout Enumerate
The ones that use several 
\emph on
Keys 
\emph default
for each
\emph on
 
\emph default
audio signal
\emph on
.
 
\emph default
Comparison between two signals is performed by measuring the amount of Keys
 that they have in common.
\end_layout

\begin_layout Standard
The second approach has proven to be more robust.
 Actually, distortions such as noise, pitch-shifting, compression etc can
 affect the construction of the fingerprints.
 In the first case, if the distortion affects the construction of the identifier
, then recognition would fail.
 
\end_layout

\begin_layout Standard
In the second case, although distortions will certainly modify some 
\emph on
Keys
\emph default
, the usual hypothesis is that 
\emph on
some of them
\emph default
 would remain the same, and sufficiently so as to still be able to identify
 the audio segment.
 This second approach is the one that now prevails in industrials solutions
 such as Shazam and so we shall adopt it here.
\end_layout

\begin_layout Subsection
Building Keys
\end_layout

\begin_layout Standard
There are different ways to build 
\emph on
keys, 
\emph default
some of them are listed below:
\end_layout

\begin_layout Subsubsection
Peaks and Thresholded values indexes
\end_layout

\begin_layout Standard
Whatever the chosen representation (STFT 
\begin_inset CommandInset citation
LatexCommand cite
key "Wang"

\end_inset

, CQT
\begin_inset CommandInset citation
LatexCommand cite
key "gretsi"

\end_inset

 or Cochleogram) local maxima can be selected and directly used as fingerprint
 keys.
 Nonetheless it's absolute time localization can not serve as a reference
 if shift-invariance is required (which is the case for most fingerprinting
 schemes).
 This is an approach investigated in 
\begin_inset CommandInset citation
LatexCommand cite
key "Moussallam2012c"

\end_inset

.
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/fingerprint_panzani_XMDCTSparseSketch.pdf
	width 8cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Visualization of sketch as a sparse collection of Time-Frequency Peaks.
 Each of these dots defines a key.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Pairs of Time-Frequency Peaks
\end_layout

\begin_layout Standard
Much more robust than isolated Peaks, Pairs of Peaks have been introduced
 by Wang in the Shazam standard 
\begin_inset CommandInset citation
LatexCommand cite
key "Wang,Wang_com,ismir,gretsi"

\end_inset

.
 The key is now defined by the relative information: 
\begin_inset Formula $(f1,\Delta f,\Delta t)$
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/fingerprint_panzani_STFTPeaksSketch.pdf
	width 8cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Visualization of Pairs of Peaks in a Time-Frequency plane.
 Each of the black segment figures a pair of peaks that constitute a key.
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Feature vectors
\end_layout

\begin_layout Standard
Along with their binarization (see 
\begin_inset CommandInset citation
LatexCommand cite
key "Haitsma,Muller2005"

\end_inset

).
 Features such as Mel-Frequency Cepstral Coefficient (MFCC) or Pitch Class
 Profile (PCP) also known as Chroma features (see also 
\begin_inset CommandInset citation
LatexCommand cite
key "Jiao2008,Philips_DCT,Scheirer1997"

\end_inset

).
\end_layout

\begin_layout Subsubsection
Formant differences
\end_layout

\begin_layout Standard
One can think of new ways to build fingerprint keys.
 For instance we are investigating keys computed using the Sine Wave Speech
 sketch.
 It is not clear how to efficiently combine the information carried by the
 formants resonant frequency variations across time.
 The simplest way to measure distance in such space would be to perform,
 say a dynamic time warping between formant series.
 However this would require too much computation time.
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/fingerprint_panzani_SWSSketch.pdf
	width 8cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Novel type of key.
 At each time step, the differences between formants F2-F1 and F1-F0 are
 computed.
 This gives a pair of frequency differences that are used as a key.
 In the graph, a key is a combination of the blue and green cross at a given
 time position.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
A much simpler way, based on keys comparison is to store the difference
 between neighboring formants as the keys.
 So far this approach is not providing competitive results as we will show
 in the experiment section.
 However, we believe that there is still room for improvements on this aspect.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
Experiments
\end_layout

\begin_layout Section*
Criteria
\end_layout

\begin_layout Standard
Fingerprinting systems are evaluated on the following criteria:
\end_layout

\begin_layout Itemize
Robustness: To Noise and distortion.
 The inability of a system to detect an excerpt as being part of the database
 is a False Negative
\end_layout

\begin_layout Itemize
Reliability: How often is an excerpt incorrectly identified: this is referred
 to as False Positive 
\end_layout

\begin_layout Itemize
Size: The number of bits required to encode a fingerprint
\end_layout

\begin_layout Itemize
Speed: Or the complexity, measured by Flops or CPU time
\end_layout

\begin_layout Itemize
Scalability: Will the system continue to perform if the database is about
 million songs
\end_layout

\begin_layout Section
Robustness experiments
\end_layout

\begin_layout Subsection
Noise Robustness
\end_layout

\begin_layout Standard
We are interested in the following problem.
 Let 
\begin_inset Formula $F[x]=\{K[x]\}_{k}$
\end_inset

 be the fingerprint of a clean signal 
\begin_inset Formula $x$
\end_inset

 as a collection of 
\begin_inset Formula $k$
\end_inset

 keys.
 Let now 
\begin_inset Formula $F[y]=F[x+w]=\{K[x+w]\}_{k}$
\end_inset

 be the fingerprint of a signal 
\begin_inset Formula $y$
\end_inset

 being the same as 
\begin_inset Formula $x$
\end_inset

 but corrupted with additive white gaussian noise 
\begin_inset Formula $w$
\end_inset

.
 Depending on the variance of 
\begin_inset Formula $w$
\end_inset

 (or equivalently the Signal To Noise ratio of 
\begin_inset Formula $y$
\end_inset

), we measure the Key Overlap Ratio:
\begin_inset Formula 
\[
KOR=\frac{\#\{K[x]\}_{k}\bigcap K[y]\}_{k}}{k}
\]

\end_inset

which is the amount of keys that have remained unchanged by the noise.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Key-Robustness-noise"

\end_inset

 shows these values for 4 different sketches/fingerprint associations:
\end_layout

\begin_layout Description
XMDCTSparse is an approximation in a 3 times redundant MDCT dictionary.
 Here 
\begin_inset Formula $k$
\end_inset

 corresponds to the number of selected atoms and the keys are directly the
 atom indexes.
\end_layout

\begin_layout Description
SWS is a sine wave speech sketchification.
 The fingerprint keys are the frequency intervals of the selected formants
 resonant frequencies.
\end_layout

\begin_layout Description
STFTPeaks is a Shazam-like mechanism.
 
\begin_inset Formula $k$
\end_inset

 is here the number of peaks selected in the STFT.
 Keys are defined by peak-pairs.
\end_layout

\begin_layout Description
CochleoPeaks is exactly the same as STFT peaks except the STFT representation
 is replaced by a cochleogram.
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/NoiseRobustness_k10.pdf
	width 8cm

\end_inset


\begin_inset Graphics
	filename figures/NoiseRobustness_k50.pdf
	width 8cm

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/NoiseRobustness_k100.pdf
	width 8cm

\end_inset


\begin_inset Graphics
	filename figures/NoiseRobustness_k200.pdf
	width 8cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Key Robustness to additive white noise.
 Values for 
\begin_inset Formula $k=10,$
\end_inset

50,100 and 200
\begin_inset CommandInset label
LatexCommand label
name "fig:Key-Robustness-noise"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Interpreting the results
\end_layout

\begin_layout Standard
High KOR values means a large proportion of keys are not affected by additive
 white Gaussian noise.
 Following remarks can be made regarding results presented in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Key-Robustness-noise"

\end_inset

:
\end_layout

\begin_layout Itemize
Unsurprisingly, the Sine Wave Speech based fingerprint appears to be the
 less robust to addition of noise.
 Indeed, the formant center frequency selection process is fairly unstable
 and minor modification of the spectral shape has large consequences on
 the SWS and therefore on the fingerprint.
 results may be expected to be even worse in the case of colored noise,
 where the spectral balance is even more affected.
 
\end_layout

\begin_layout Itemize
Cochleogram-based pairs of peaks appears to have a higher robustness to
 (white Gaussian) noise than STFT-based ones.
 Although additional tests must be conducted before strong conclusion can
 be made, one can already see that the auditory model builds a representation
 that is slightly less affected by white Gaussian noise.
 This may not be the case with colored noise, especially if it is designed
 to mainly affect frequency regions that are emphasized by the cochlear
 response model.
\end_layout

\begin_layout Itemize
Sparse approximation based sketches are clearly the most robust to additive
 noise.
 This also is no surprise since the feature, in that case, are specifically
 selected for this purpose.
 This can also be linked with conclusion of the Sketch I study 
\begin_inset CommandInset citation
LatexCommand cite
key "Suied2013"

\end_inset

.
\end_layout

\begin_layout Itemize
In general, increasing 
\begin_inset Formula $k$
\end_inset

 decreases the KOR score.
 This emphasizes the fact that the first selected features are the most
 robust ones.
\end_layout

\begin_layout Subsection
Invariance robustness
\end_layout

\begin_layout Standard
We are now interested in the following problem.
 Let 
\begin_inset Formula $F[x]=\{K[x]\}_{k}$
\end_inset

 be the fingerprint of a signal 
\begin_inset Formula $x$
\end_inset

 as a collection of 
\begin_inset Formula $k$
\end_inset

 keys.
 Let now 
\begin_inset Formula $F[y]=F[x*\delta_{\tau}]=\{K[x*\delta_{\tau}\}_{k}$
\end_inset

 be the fingerprint of a signal 
\begin_inset Formula $y$
\end_inset

 being the same as 
\begin_inset Formula $x$
\end_inset

 but translated in time.
 Again, we can measure the robustness using the KOR value.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/ShiftRobustness_k50.pdf
	width 8cm

\end_inset


\begin_inset Graphics
	filename figures/ShiftRobustness_k200.pdf
	width 8cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Key Robustness to time shifting.
 Values for 
\begin_inset Formula $k=50$
\end_inset

 and 200
\begin_inset CommandInset label
LatexCommand label
name "fig:Key-Robustness-shift"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Interpreting the results
\end_layout

\begin_layout Standard
High KOR values means a large proportion of keys are not affected by time
 shifts.
 Following remarks can be made regarding results presented in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Key-Robustness-shift"

\end_inset

:
\end_layout

\begin_layout Itemize
Sine Wave Speech results are hard to interpret, but they emphasize the fact
 that a large number of keys are not discriminant enough.
\end_layout

\begin_layout Itemize
Cochleogram and STFT based pairs of peaks have fairly comparable performances.
 One can see that increasing the number of keys increases the robustness
 in this case.
 The oscillatory pattern stresses the fact that peak selection is highly
 phase dependent.
 More robustness could be attained by further quantifying the frequencies
 of the keys.
\end_layout

\begin_layout Itemize
Sparse approximation based sketches are once again quite robust to the deformati
on.
\end_layout

\begin_layout Section
Results on recognition tasks
\end_layout

\begin_layout Standard
Robustness measure are not sufficient to compare fingerprinting systems.
 Not only must a fingerprint be robust to deformation, but it should also
 be characteristic of a sound.
 The discriminative power of a fingerprint can only be evaluated by large
 scale experiments.
\end_layout

\begin_layout Subsection
Fingerprint Test framework
\begin_inset CommandInset label
LatexCommand label
name "sub:Fingerprint-Test-framework"

\end_inset


\end_layout

\begin_layout Standard
Any test run consists in two phases:
\end_layout

\begin_layout Enumerate
Learning: an audio dataset 
\begin_inset Formula $\mathcal{D}$
\end_inset

 is a collection of 
\begin_inset Formula $T$
\end_inset

 tracks 
\begin_inset Formula $X_{t}$
\end_inset

.
 Each track is cut into (non-overlapping) 
\begin_inset Formula $S_{t}$
\end_inset

 segments of 5 seconds.
 Each segment 
\begin_inset Formula $x_{s,t}$
\end_inset

 is then processed by the fingerprint system which yields a collection of
 
\begin_inset Formula $k$
\end_inset

 keys 
\begin_inset Formula $\{K[x_{s,t}]\}_{k}$
\end_inset

 and associated values.
 Each key is then hashed and stored in a dedicated database 
\begin_inset Formula $\mathcal{B}$
\end_inset

 on the disk.
 Two cases arises:
\end_layout

\begin_deeper
\begin_layout Enumerate
The key does not exist in the database.
 Then the key is added and the value associated is the place of occurrence,
 i.e.
 the time of occurrence and the track index 
\begin_inset Formula $t$
\end_inset


\end_layout

\begin_layout Enumerate
The key already exists in the database.
 Then the place of occurrence is added to the list of values associated
 with this key.
\end_layout

\begin_layout Standard
At the end of this process, the dataset has been entirely transformed into
 a database of fingerprints
\end_layout

\end_deeper
\begin_layout Enumerate
Testing: for each track, random segments of 5 seconds are selected.
 These segments are not aligned with the ones of the learning phase.
 For each segment, the fingerprint is computed, yielding a collection of
 keys.
 For each key, the associated values are retrieved from the database 
\begin_inset Formula $\mathcal{B}$
\end_inset

 and a majority vote is conducted to decide of the most probable place of
 occurrence.
 For each segment there is three kind of result:
\end_layout

\begin_deeper
\begin_layout Enumerate
Segment is correctly identified as track 
\begin_inset Formula $t$
\end_inset

 and in the correct time position
\end_layout

\begin_layout Enumerate
Segment is correctly identified as track 
\begin_inset Formula $t$
\end_inset

 but with the wrong time position.
 
\end_layout

\begin_layout Enumerate
Segment is incorrectly identified as track 
\begin_inset Formula $t'$
\end_inset


\end_layout

\end_deeper
\begin_layout Paragraph
Metrics
\end_layout

\begin_layout Standard
Performances are simply expressed in terms of 2 recognition rates:
\end_layout

\begin_layout Itemize
Conservative: the ratio of segments whose both track and time of occurrence
 have been correctly found
\end_layout

\begin_layout Itemize
Relaxed: the ratio of segments whose track have been correctly found
\end_layout

\begin_layout Subsection
Results on real datasets
\end_layout

\begin_layout Paragraph
Setup: GTZAN dataset (1000 songs of 30 seconds each 
\begin_inset Formula $\sim$
\end_inset

8.3 hours).
 Segments of 5 seconds.
\end_layout

\begin_layout Standard
The GTZAN dataset contains 100 musical examples of 10 different musical
 genres (reggae, rock , jazz, classical, etc..).
 Is is known to contain some mistakes, but is a good framework for our analysis
 since many of the pieces it contains are similar in content (at least they
 belong to the same musical genre).
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/GTZAN_Scores_2fgpts_dur5.pdf
	width 12cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Recognition scores on the GTZAN dataset with STFT and Cochleograms pairs
 of peaks.
 Dashed lines are conservative recognition rates.
\begin_inset CommandInset label
LatexCommand label
name "fig:RecognitionGTZAN"

\end_inset

.
 Computation times achieved on a Xeon(R) CPU X5482 @ 3.20GHz running a Linux/Debi
an system with 32Gb of RAM.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:RecognitionGTZAN"

\end_inset

 shows results in terms of recognition rate.
 The test framework described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Fingerprint-Test-framework"

\end_inset

 has been applied to fingerprint systems using pairs of peaks in STFT and
 Cochleograms.
 Runs where initialized with varying number of peaks.
 Increasing the number of peaks has three effects: 
\end_layout

\begin_layout Enumerate
It increases the size of the database that contains all the key/values of
 the fingerprints
\end_layout

\begin_layout Enumerate
It increases the computation time
\end_layout

\begin_layout Enumerate
It enhances the recognition capabilities since more keys (usually) means
 more discriminative fingerprints.
\end_layout

\begin_layout Paragraph
Setup: RWC sub dataset (80 songs of a few minutes each 
\begin_inset Formula $\sim$
\end_inset

 5.5 hours).
 Segments of 5 seconds.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/RWCLearn_Scores_2fgpts_dur5.pdf
	width 12cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Recognition scores on a subset of the RWC pop dataset with STFT and Cochleograms
 pairs of peaks.
\begin_inset CommandInset label
LatexCommand label
name "fig:Recognition-RWC"

\end_inset

 Computation times achieved on a Xeon(R) CPU X5482 @ 3.20GHz running a Linux/Debi
an system with 32Gb of RAM.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Ongoing Experiments 
\end_layout

\begin_layout Subsection
Extensive comparison with as many sketches as possible
\end_layout

\begin_layout Standard
As we have presented above, their are numerous ways of computing sketches
 and numerous ways of computing fingerprints out of those sketches.
 So far, satisfactory results have been achieved only with fingerprinting
 schemes based on peak pairs on STFT and Cochleograms, however the following
 schemes are under development and almost finished:
\end_layout

\begin_layout Itemize
Fingerprints based on sparse approximations
\end_layout

\begin_layout Itemize
Fingerprints on the various sub parts of the corticogram
\end_layout

\begin_layout Itemize
Fingerprints on a plain CQT (L.
 Cornu)
\end_layout

\begin_layout Standard
Designing an efficient fingerprint scheme for the Sine Wave Speech is a
 more challenging task that we are currently concentrated on.
 Indeed, the instability of the formant selection process is a major technical
 difficulty.
 We do not expect such fingerprints to be competitive on the same datasets
 as tested above.
 However, datasets of pure speech signals may give very different results
 and be more coherent with the signal model at the core of the SWS idea.
\end_layout

\begin_layout Subsection
Feedback and statistical modeling
\end_layout

\begin_layout Standard
The experimental setup presented so far is a direct application of the commonly
 found solution to the automatic recognition by acoustic fingerprint comparison
 task.
 As we've seen, a sketch is computed for a given sound following a fixed,
 deterministic process.
 The adequacy of a sketch to the retrieval task plays no role in its constructio
n, although it is really what is important here.
\end_layout

\begin_layout Standard
In other word, we need to define sketches that may be less descriptive and
 more discriminative of the sounds.
 To do so we need to learn statistical models for the sketches.
 In particular, adopting the dictionary-based point of view, the problem
 should not only be to find the features 
\begin_inset Formula $\phi$
\end_inset

 that are good at describing a signal 
\begin_inset Formula $x$
\end_inset

, but only the ones that are not easily found in other signals.
 This can be written as a problem of the form:
\begin_inset Formula 
\[
\min_{\alpha}\|\alpha\|_{0}\mbox{ such that }\|x-\sum\alpha_{i}\phi_{i}\|^{2}\leq\epsilon\mbox{ and }\max D(\alpha,\alpha_{common})
\]

\end_inset

with 
\begin_inset Formula $D(\alpha,\alpha_{common})$
\end_inset

 being a distance between the chosen 
\begin_inset Formula $\alpha$
\end_inset

 and a prototype 
\begin_inset Formula $\alpha_{common}$
\end_inset

 that can be learned on the dataset.
\end_layout

\begin_layout Standard
This suggests a feedback mechanism.
 Sketches are computed and evaluated in the retrieval task, a prototype
 is build that in turn serves for the computation of the sketched and so
 on.
 Such mechanism can also be seen as a genetic algorithm with the fingerprint
 evaluation being the fitness function.
 The next few months should be dedicated to building this kind of framework.
\end_layout

\begin_layout Subsection
Longer-term
\end_layout

\begin_layout Standard
The automatic recognition by fingerprint comparison task is a well defined
 problem that allows empirical comparison between different schemes.
 However, its relationship with the 
\emph on
priming 
\emph default
issue is not clearly known.
 In the longer-term, it would be nice to design an experiment inspired by
 the fingerprint problem that reflects the 
\emph on
priming 
\emph default
effect.
\end_layout

\begin_layout Standard
A few ideas come to mind.
 Fingerprints could be stored in a cache memory, allowing faster retrieval
 of sounds in the hash tables.
 Nonetheless, designing a precise framework that mimics the mammal brain
 behavior remains a challenging task.
 The follow-up of this project (e.g.
 WP 2.1 and 2.2) should shed some light on this topic.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
Implementation details
\end_layout

\begin_layout Section*
Language and libraries
\end_layout

\begin_layout Standard
A unified framework has been developed in Python (2.7 syntax).
 It has been tested on Linux/Ubuntu, Windows 32 bits and Mac OS X platforms.
 The manipulation of audio signals, spectrogram construction, and sparse
 greedy algorithms are performed through the use of the PyMP
\begin_inset Foot
status open

\begin_layout Plain Layout
Download at https://github.com/mmoussallam/PyMP
\end_layout

\end_inset

 library, for which I have written some tutorials
\begin_inset Foot
status open

\begin_layout Plain Layout
At: http://manuel.moussallam.net/PyMP/
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
Everywhere it is possible, the code is run in parallel using the 
\emph on
joblib 
\emph default
python package.
 The hash tables manipulation is performed in pure C via a python wrapper.
 
\end_layout

\begin_layout Subsection*
Versionning
\end_layout

\begin_layout Standard
All the source code developed in this project so far is versioned using
 
\emph on
git 
\emph default
and is stored in a (private and secured) repository on github.
 The code is object-oriented as much as possible, which means the sounds,
 sketches, representation, fingerprints and databases are Python objects.
 Experiments are plain python scripts.
\end_layout

\begin_layout Paragraph
Tests
\end_layout

\begin_layout Standard
Following principles of test-driven development, unit testing is performed
 on the code using python 
\emph on
unittest
\emph default
 package.
 this guarantees back compatibility and ensure no critical part is damaged
 after code edition.
 They are located in the tests package.
\end_layout

\begin_layout Paragraph
Speech analysis
\end_layout

\begin_layout Standard
For computing the sws, we use the praat
\begin_inset Foot
status open

\begin_layout Plain Layout
Doc and download at http://www.fon.hum.uva.nl/praat/
\end_layout

\end_inset

 software since it has a built-in formant tracking algorithm.
 It is not very handy to use but it might be the fastest way for now.
 If Sws appears to be more interesting in the future we may have to re-implement
 this part.
 Praat is stable on linux based system but I haven't tried it on windows
 platforms so far.
\end_layout

\begin_layout Section*
Sketches
\end_layout

\begin_layout Standard
All sketches inherit from an abstract interface AudioSketch.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:UML-diagram-sketches"

\end_inset

 shows a UML diagram of the sketches that have been implemented so far.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../classes/classes_Sketches.png
	lyxscale 20
	width 16cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
UML diagram of the sketches objects.
 There is a unique abstract interface for audio sketches.
\begin_inset CommandInset label
LatexCommand label
name "fig:UML-diagram-sketches"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
This architecture allows an easy manipulation of the different sketches
 and is crucial for large scale experiments.
\end_layout

\begin_layout Section*
Fingerprints
\end_layout

\begin_layout Standard
All fingerprinting systems uses the same interface as depicted in Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:UML-Fingerprints"

\end_inset

.
 The super class makes a single interface with the Berkeley DB (BDB) API.
 BDB objects must be manipulated with great care since they correspond to
 low-level memory and disk file-system editing.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../classes/classes_FgptHand.png
	lyxscale 20
	width 8cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
UML diagram of the fingerprint handler objects.
 There is a unique abstract interface that deals with the low level hash
 table system.
\begin_inset CommandInset label
LatexCommand label
name "fig:UML-Fingerprints"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
This architecture allows to develop specific fingerprinting schemes without
 worrying about the low-level handling of BDB objects anymore.
 
\end_layout

\begin_layout Subsection*
Berkeley DB database
\end_layout

\begin_layout Standard
Berkeley DB is a widely used C library that implements hash tables and relationa
l databases.
 When properly configured, it can handle many write and read access to a
 database stored in RAM and flush to the disk, say, at the end of the learning
 stage.
 Its configuration can be tricky.
 Typical efficient usage may include:
\end_layout

\begin_layout Itemize
Storage in RAM for fast read-only access (for instance in a fingerprinting
 
\emph on
Test
\emph default
 phase).
 Enough heap space must be allocated in this mode
\end_layout

\begin_layout Itemize
Storage on disk between experiments.
 The hash table structure can be nicely flushed to disk between fingerprinting
 phases and experiments.
 
\end_layout

\begin_layout Itemize
Fast multi-threaded read/write access (for instance in a fingerprinting
 
\emph on
Learn
\emph default
 phase).
 The multi-threading must be carefully handled.
 This mode is the most sensitive one and is unlikely to be easily efficiently
 implemented on all platforms.
 However it is easy to downgrade to a safe, single-threaded mode with pure
 RAM manipulation.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "/home/manu/Documents/Global-Fingerprint,/home/manu/Documents/Global-Fingerprint-sketch,/home/manu/Documents/Global-Perception,/home/manu/Documents/Global-Own,/home/manu/Documents/Global"
options "bibtotoc,abbrv"

\end_inset


\end_layout

\end_body
\end_document
