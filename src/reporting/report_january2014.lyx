#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass report
\use_default_options true
\begin_modules
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement H
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 3cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 2
\paperpagestyle headings
\bullet 0 1 1 -1
\bullet 1 0 31 -1
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Technical report on project Sketch II
\end_layout

\begin_layout Date
dec.
 2013
\end_layout

\begin_layout Author
L.
 Cornu, M.
 Moussallam, L.
 Daudet
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section*
Context of this study
\end_layout

\begin_layout Standard
This document summarizes the technical details of the framework built as
 part of the Sketch 2 (a.k.a.
 Ghost Sounds) project.
 More precisely, it is part of the work-packages (WP) 1.1 and 1.2.
 For the sake of readability and clarity, the amount of technical details
 is limited to what is necessary for apprehending the basics concepts.
\end_layout

\begin_layout Subsection*
Goals and Tasks
\end_layout

\begin_layout Standard
The first phase of the project aims at building a 
\begin_inset Quotes eld
\end_inset

sketchifying
\begin_inset Quotes erd
\end_inset

 process tailored for priming experiments.
 The deliverable should take the form of a reusable piece of software that
 is able to build a 
\begin_inset Quotes eld
\end_inset

sketch
\begin_inset Quotes erd
\end_inset

 from any given audio scene.
\end_layout

\begin_layout Itemize
WP 1.1 consists in the definition and characterization of audio 
\begin_inset Quotes eld
\end_inset

sketches
\begin_inset Quotes erd
\end_inset

 via a signal and/or a statistical model.
 We are interested in finding not only the main features of sounds, but
 more importantly their peculiar ones.
 In order to do so, one must first decide of a feature space on which to
 project the signals, and second, distinguish which of these feature (or
 sets of features) are discriminative.
\end_layout

\begin_layout Itemize
WP 1.2 addresses the problem of validation via an automatic recognition task.
 The recognition is performed by means of acoustic fingerprints comparison.
 This research field being a mature one, the objective can not be to compete
 with existing industrial solution.
 Nonetheless, those should serve as a basis for benchmarking any proposed
 fingerprint scheme.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
The goal of this task is to build a sketches model for a specific class
 of sounds under study (for instance in the statistics of selected features,
 and maybe more importantly in their pairwise correlation).
 From this model and a given sound, we can now investigate how can we build
 a sound “fingerprint” that is as compact as possible, and that uniquely
 identifies this sound.
 From an information theory standpoint, the selected features would not
 be the ones that simply best describe the sound, but the ones that best
 describe the sound knowing the sound model, in other words the features
 that distinguish the individual from the group.
 To come back to the original visual metaphor, we do not desire a realistic
 “sketch” anymore, but a voluntarily distorted “caricature” that emphasizes
 the specifics of the original object and forgets about features that may
 be large in absolute terms but in standard proportion in the average model
 (when we want to describe a person’s face, we do not start by saying “he
 has a mouth, a nose and two eyes”, as these are important but obvious features,
 we only focus on small specific details “he has a square chin and large
 ears”).
 Fundamentally speaking, this can be seen as a generalized notion of sound
 timbre, not as an instrinsic quality of the sound, but as a notion relative
 to the other sounds (the original notion of timbre is the set of characteristic
s that allow us to distinguish two sounds that have the same pitch, duration,
 and loudness).
 
\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
Object and concept definitions
\end_layout

\begin_layout Description
Sounds Any digital audio signal, as the discrete sampling of an acoustic
 wave.
 For simplicity's sake, the spatial properties may be discarded and only
 mono-channel signals may be considered.
 Among properties one may find the sampling frequency and the length or
 duration.
\end_layout

\begin_layout Description
Track An audio track is a long sound (in the order of 30 seconds to several
 minutes) that may for instance be a musical interpretation.
 Among properties one may find the title, artist name, duration, musical
 genre or speaker id.
\end_layout

\begin_layout Description
Scene An audio scene is a short sound, e.g.
 a single sentence uttered by a speaker or an environmental sound such as
 a car passing, or a precise part in an audio track (e.g.
 chorus or verse)
\end_layout

\begin_layout Description
DataSet A collection of sounds (or tracks).
\end_layout

\begin_layout Description
Sketch A simplified version of an audio scene.
 By simplify we mean any way to approximate the original object via dimensionali
ty reduction.
 We assume the reader is aware of the Sketch 1 paper 
\begin_inset CommandInset citation
LatexCommand cite
key "Suied2013"

\end_inset

.
\end_layout

\begin_layout Description
Fingerprint A hash version of a sketch, that can be stored and retrieved
 efficiently from a fingerprint database
\end_layout

\begin_layout Description
Fingerprint
\begin_inset space ~
\end_inset

Database A hashtable of key/value pairs built from the acoustic fingerprints
 of an entire DataSet.
 The storing and retrieval is facilitated either by an efficient hash indexing
 or a tree structure.
 
\end_layout

\begin_layout Subsection*
Participants
\end_layout

\begin_layout Standard
The project officially started in march 2013.
 WP 1.1 and 1.2 were conducted by Pr.
 L.
 Daudet, Dr.
 M.
 Moussallam and L.
 Cornu.
\end_layout

\begin_layout Chapter
Representation of sound
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Manu
\end_layout

\end_inset


\end_layout

\begin_layout Section
Time-Frequency Representations
\end_layout

\begin_layout Subsection
Linear Fourier : STFT
\end_layout

\begin_layout Standard
The standard Short-Term Fourier Transform (STFT) is a lapped transform using
 a standard Fourier transform on overlapping (often windowed) segments.
 Let 
\begin_inset Formula $w$
\end_inset

 be a window of size 
\begin_inset Formula $L$
\end_inset

, the STFT of 
\begin_inset Formula $x$
\end_inset

 is indexed in time by the frame (i.e.
 segment) index 
\begin_inset Formula $p$
\end_inset

 (
\begin_inset Formula $0\leq p\leq P-1)$
\end_inset

 and in frequency by 
\begin_inset Formula $k$
\end_inset

 (
\begin_inset Formula $0\leq k\leq k-1)$
\end_inset

:
\begin_inset Formula 
\[
R_{STFT}(x)[p,k]=\frac{1}{\gamma}\sum_{n=0}^{L-1}w[n]\cdot x[n-n_{p}]\cdot\exp\left(-2j\pi k\frac{n}{L}\right)
\]

\end_inset

where 
\begin_inset Formula $\gamma$
\end_inset

 is a normalization factor and 
\begin_inset Formula $n_{p}$
\end_inset

 is the step between two consecutively analyzed frames.
 The STFT is a complex representation, in order to visualize and /or manipulate,
 one usually consider the logarithm of its magnitude.
 This is referred to as the spectrogram of 
\begin_inset Formula $x$
\end_inset

:
\begin_inset Formula 
\[
R_{Spectrogram}(x)=\log\left(|R_{STFT}(x)|\right)
\]

\end_inset

properties:
\end_layout

\begin_layout Enumerate
The STFT has a fixed and linear both in time and frequency resolution.
\end_layout

\begin_layout Enumerate
The STFT is perfectly invertible, provided the Constant Overlap-Add condition
 is fulfilled, meaning basically that the windows are smooth enough and
 must sum to one.
 In practice this is obtained by choosing a Hann window for 
\begin_inset Formula $w$
\end_inset

 and ensuring an overlap of 50% or more (i.e.
 
\begin_inset Formula $n_{p}\leq\frac{L}{2}$
\end_inset

).
\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /home/manumouss/workspace/audio-sketch/src/reporting/figures/exemple_stft.pdf
	width 8cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Spectrogram (Logarithm of the magnitude of the Short Time Fourier Transform)
 of an audio excerpt.
 Darker regions are more energetic.
 The Time/Frequency resolution obeys a compromise that is controlled by
 the analysis window size.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Subsection
Logarithmic: CQT
\end_layout

\begin_layout Standard
The constant-Q transform is a signal Time/Frequency representation in which
 the frequency scale is logarithmic.
 The CQT can be considered as a bank of filters whose quality factor Q remains
 constant:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathrm{Q=\frac{f_{k}}{\Delta f}}
\]

\end_inset


\end_layout

\begin_layout Standard
with 
\begin_inset Formula $\mathrm{f_{k}}$
\end_inset

 being the natural frequency of the filter 
\begin_inset Formula $k$
\end_inset

 and 
\begin_inset Formula $\mathrm{\Delta f_{k}}$
\end_inset

 its bandwidth.
 In order to keep the quality factor Q constant over the different frequencies,
 the lengths 
\begin_inset Formula $L_{k}$
\end_inset

 of the filters are dyadic and inversely proportional to the resonant frequencie
s .
 The representation is defined by:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
R_{CQT}(x)[k]=\frac{1}{L_{k}}\sum_{n=0}^{^{L_{k}-1}}w_{k}[n].x[n]exp\left(-2j\pi n\frac{f_{k}}{f_{s}}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $w_{k}$
\end_inset

 is a Hann window determining the length of the filter centred in 
\begin_inset Formula $f_{k}$
\end_inset

 and 
\begin_inset Formula $f_{s}$
\end_inset

 is the sampling rate.
\end_layout

\begin_layout Standard
The frequency scale can be chosen in order to fit the MIDI musical scale,
 then intervals correspond to the semitone scale (frequencies of notes in
 a 12-tone scale).
 That property make this representation useful in musical signal analysis
 
\begin_inset CommandInset citation
LatexCommand cite
key "gretsi,Fuentes2012"

\end_inset

.
 The note A4, witch correspond to the frequency 440Hz is defined by the
 index 69 in the MIDI scale, by using the relation above, we fit the frequency
 scale to the MIDI scale:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
midi(f)=69+12log_{2}\left(\frac{f}{440}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/cqtIHT.png
	scale 50
	rotateOrigin center

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Constant Q Transform (magnitude of the output of constant-Q filters) of
 the sentence 'we will have to watch our chances'.
 Darker regions are more energetic.
 The Time/Frequency resolution depends on the frequency.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Recent works have shown that the CQT can be inverted 
\begin_inset CommandInset citation
LatexCommand cite
key "Fillon2012"

\end_inset

.
 The problem is that to keep the property of invertibility, the temporal
 step between two analyzed frames have to be smaller than the half-window
 of the higher frequency (which can be very small, about 1millisecond for
 a frequency of 16kHz).
 In practice, this implies heavy computational costs and the inverse is
 usually approximated.
\end_layout

\begin_layout Subsection
Cochleograms
\end_layout

\begin_layout Standard
The auditory system is thought to contain an array of over-lapping band-pass
 filters known as ‘auditory filters’ 
\begin_inset CommandInset citation
LatexCommand cite
key "Yang1992,Wang1995"

\end_inset

.
 They occur along the basilar membrane and increase the frequency selectivity
 of the cochlea and therefore the listener’s discrimination between different
 sounds.
 Cochleograms model the output of the auditory filters.
 This transformation is mainly similar that the one use for the CQT, but
 using a real auditory response filter as the filter banks.
 The difference is the non-symmetricalness of auditory filters.
 A auditory filter centered in the frequency 
\begin_inset Formula $f_{c}$
\end_inset

 is writing in the time domain as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
gt(t)=at^{(n-1)}exp\left(-2\pi bt\right)cos\left(2\pi f_{c}t+\phi\right)
\]

\end_inset


\end_layout

\begin_layout Standard
It is basically a cosine windowed by a non-symmetric widows that is the
 product of a power term and a decreasing exponential.
 The parameters of each filters mainly control the duration end therefore
 the bandwidth.
\begin_inset space ~
\end_inset

A nice introduction to cochleograms can be found in 
\begin_inset CommandInset citation
LatexCommand cite
key "Viberg2004"

\end_inset

 and further details in 
\begin_inset CommandInset citation
LatexCommand cite
key "Yang1992,Wang1995"

\end_inset

.
 In this study we used the same set of coefficients filter as in existing
 software (i.e the NSL toolbox).
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/cochleoPP.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Cochleogram (absolute value) of the sentence 'we will have to watch our
 chances'.
 Darker regions are more energetic.
 The frequency scale is logarithmic and the Time/Frequency resolution is
 frequency dependent.
 according to the human sensitivity, low and high frequencies magnitude
 are lowered.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The Cochleogram is the representation in Time/Frequency of the output of
 the magnitude of auditory filters.
 We can see in the next figure that they emphasize the magnitude over the
 frequency spectrum according to the human sensitivity of tones.
 
\end_layout

\begin_layout Subsection
Sparse Multi-Resolution Approximation
\end_layout

\begin_layout Standard
Corticograms can be considered as a multi-resolution view of cochleograms.
 A different class of multi-resolution representations can be built using
 the theory of frames 
\begin_inset CommandInset citation
LatexCommand cite
key "Mallat2009"

\end_inset

 and more generally using a union of analysis 
\emph on
dictionaries
\emph default
.
 A dictionary 
\begin_inset Formula $\Phi$
\end_inset

 is any family of 
\begin_inset Formula $M$
\end_inset

 vectors 
\begin_inset Formula $\phi_{i}$
\end_inset

.
 One is typically interested in solving a synthesis problem of finding the
 coefficients 
\begin_inset Formula $\alpha_{i}$
\end_inset

 such that:
\begin_inset Formula 
\begin{equation}
x=\sum_{i=0}^{M-1}\alpha_{i}\phi_{i}\label{eq:synthesis-dico}
\end{equation}

\end_inset

which poses no problem when the dictionary is an orthonormal basis.
 But in order to achieve multi-resolution, one might built a dictionary
 as a union of orthonormal basis (for instance, STFT of different window
 length or MDCT).
 However, in such 
\emph on
overcomplete
\emph default
 dictionaries, their might not be a unique set of coefficient satisfying
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:synthesis-dico"

\end_inset

.
 The problem of 
\emph on
sparse representation
\emph default
 therefore consists in finding, among all possible solutions the one that
 minimizes a 
\emph on
sparsity cost function, 
\emph default
usually the 
\begin_inset Formula $\ell_{0}$
\end_inset

-pseudo norm.
 The sparse 
\emph on
representation
\emph default
 problem thus writes:
\begin_inset Formula 
\begin{equation}
\min_{\alpha}\|\mathbf{\alpha}\|_{0}\mbox{ such that }x=\sum_{i=0}^{M-1}\alpha_{i}\phi_{i}\label{eq:l0_prob}
\end{equation}

\end_inset

and its sparse 
\emph on
approximation
\emph default
 counterpart:
\begin_inset Formula 
\begin{equation}
\min_{\alpha}\|\mathbf{\alpha}\|_{0}\mbox{ such that }\|x-\sum_{i=0}^{M-1}\alpha_{i}\phi_{i}\|^{2}\leq\epsilon\label{eq:l0_approx}
\end{equation}

\end_inset

which are (most of the time) NP-hard to solve.
 Potentially sub-optimal solutions are found using selection mechanism such
 as thresholding or greedy algorithms.
 There is a huge literature on these methods (for instance 
\begin_inset CommandInset citation
LatexCommand cite
key "Mallat_TSP1993,Chen98atomicdecomposition,Donoho2006a"

\end_inset

), but for practical purposes, one can look into the PyMP tutorials
\begin_inset Foot
status open

\begin_layout Plain Layout
http://manuel.moussallam.net/PyMP/
\end_layout

\end_inset

.
\end_layout

\begin_layout Section
Scale-Space Representations
\end_layout

\begin_layout Subsection
Wavelet-based transform
\end_layout

\begin_layout Standard
As we seen in the previous part, the resolution of time and frequency in
 a STFT representation obey to a firm compromise established for the need
 of analysis.
 In order to get a good resolution in time we need to use sharp windows
 that will affect the resolution of frequencies.
 The inverse is true, a good resolution in frequencies imply a long analyzed
 window decreasing the time resolution.
 But having a good resolution for the both of them is not possible, this
 property is related to the Heisenberg uncertainty principle.
 Somehow the wavelet were developed to resolve this problem 
\begin_inset CommandInset citation
LatexCommand cite
key "Mallat2009"

\end_inset

.
 
\end_layout

\begin_layout Standard
The wavelet analysis was introduced by J.
 Morlet.
 It is not based on a representation of frequencies but in transitory component
 over time.
 In this idea, the wavelet transform decompose the signal in different scaled
 and translated versions of a 
\shape italic
mother wavelet
\shape default
.
 The mother wavelet is a function 
\begin_inset Formula $\psi$
\end_inset

 
\begin_inset Formula $\in L\mathbf{^{2}(\mathrm{\mathbb{R}})}$
\end_inset

, zero mean, normalized, centered in 
\begin_inset Formula $t=0$
\end_inset

, the shape is adapted on what we want to analyze.
 In audio signal we often use a windowed sinusoid.
\end_layout

\begin_layout Standard
A set of Frequency/Time atoms, are obtained by a scaling 
\begin_inset Formula $s$
\end_inset

, and a translation 
\begin_inset Formula $u$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\psi_{u,s}(t)=\frac{1}{\sqrt{s}}\psi\left(\frac{t-u}{s}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
The weight of each of these scaled and translated version of the mother
 wavelet form the wavelet transform of the signal.
 It is a function of two variables, time and scale and give an alternative
 representation on 
\begin_inset Formula $f_{t}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Wf(u,s)=\langle f,\psi_{u,s}\rangle=\int_{-\infty}^{+\infty}f(t)\frac{1}{\sqrt{s}}\psi^{*}\left(\frac{t-u}{s}\right)dt
\]

\end_inset


\end_layout

\begin_layout Standard
because of the infinite sum this representation is very redundant.
 To avoid this we use discrete time and scale variables.
 Choosing 
\begin_inset Formula $k$
\end_inset

 and 
\begin_inset Formula $j$
\end_inset

, the discrete variables 
\begin_inset Formula $s=2^{j}$
\end_inset

 and 
\begin_inset Formula $u=k2^{j}$
\end_inset

, we can find a mother wavelet 
\begin_inset Formula $\psi(t)$
\end_inset

 such that the wavelet set form a orthonormal basis of the function in 
\begin_inset Formula $\mathcal{C^{\mathrm{2}}}$
\end_inset

.
\end_layout

\begin_layout Standard
If the input is a two dimensional data the wavelet basis over the continuous
 domain is the union of translating and dilating three mother wavelet functions
 
\begin_inset Formula $\left\{ \psi^{V},\psi^{H},\psi^{D}\right\} $
\end_inset

.
\end_layout

\begin_layout Standard
Each wavelet atom is defined as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\psi_{k,j,n}(t)=\psi_{k,j}^{n}(t)==\frac{1}{\sqrt{2^{j}}}\psi^{n}\left(\frac{t-k2^{j}}{2^{j}}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $s=2^{j}$
\end_inset

 , and the translation is 
\begin_inset Formula $u=2^{j}(k_{1},k_{2})$
\end_inset

, the computation do all the inner products .
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/lenaWavelet.pdf
	height 8cm
	BoundingBox 0bp 0bp 595bp 842bp
	rotateOrigin center

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Wavelet transform of an image, each plot can be understand as output of
 an inner product with a specific scaled, translated wavelet.
 They are tree different mother wavelets, 
\begin_inset Formula $\left\{ \psi^{V},\psi^{H},\psi^{D}\right\} $
\end_inset

 to form an orthogonal basis, respectively vertical, horizontal and diagonal.
 From the bottom right to the top left, the output of inner products with
 signal and scaled wavelet are ranked from the broadest scale to the sharpest.
 We observed that the representation with broad scaled wavelet show blurred
 contour of discontinuty.
 This representation is obtained by using the Toolbox image.
\begin_inset CommandInset label
LatexCommand label
name "fig:Wavelet-transform"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
A mother wavelet with 
\begin_inset Formula $n$
\end_inset

 vanishing moments is proven to be the n-derivative of a function 
\begin_inset Formula $\theta$
\end_inset

.
 This means that the wavelet transform can be seen as multi scale differential
 operator.
 As a derivative in 1D, the local regularity is characterized by decreasing
 wavelets coefficients and local irregularity is emphasize, that what we
 see in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Wavelet-transform"

\end_inset

 for the horizontal, vertical and diagonal irregularities.
\end_layout

\begin_layout Subsection
Corticograms
\end_layout

\begin_layout Standard
The primary auditory cortex is the part of the brain responsible of the
 understanding and segregation of equally tuned sound with temporal structure
 dissimilarity.
 After the cochlea processing the sound perceived are ordered along the
 tonotopic axis, witch means that they are projected according the cochlea
 shape in particular area of brain, ranking according their wavelength .
 
\end_layout

\begin_layout Standard
Then the primary cortex analyzed the spectro-temporal content of the sound
 using layers of neuron.
 Those neurons, act as bank of filters, centered in each frequencies of
 the tonotopic axis.
 Each filter, called STRF (spectro temporal response field) is tuned at
 a specific rate (temporal modulation in Hertz) and scale (frequency modulation
 in cycle/octave).
 Those actions can be approximately seen as a bank of 2D wavelet transform
 
\begin_inset CommandInset citation
LatexCommand cite
key "Wang1995"

\end_inset

.
 
\end_layout

\begin_layout Standard
The STRF shows the exhibiting and inhibitory fields are designed by 
\shape italic
ripples 
\shape default
(sound with a spectral pattern that have a sinusoidal shape along the logarithmi
c frequency axis).
 The mathematical model of the STRF is shown below:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
STRF=h_{IRT}(t)\star h_{IRS}(x)
\]

\end_inset

where 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $ $
\end_inset


\begin_inset Formula $h_{IRT}(t)$
\end_inset

 and 
\begin_inset Formula $h_{IRS}(x)$
\end_inset

 are two ripples parametrized by the characteristics of a specific neuron.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
h_{IRT}(t,w,\theta)=h_{t}(t;w)cos\theta+\hat{h_{t}}(t;w)sin\theta
\]

\end_inset

where 
\begin_inset Formula $w$
\end_inset

 and
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 
\begin_inset Formula $\Omega$
\end_inset

 are the rates and scales, and 
\begin_inset Formula $\phi$
\end_inset

, 
\begin_inset Formula $\theta$
\end_inset

 are the characteristics phases of asymmetry.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
r\pm(t,f;w,\Omega;\theta,\phi)=z(t,f)\star_{t,f}STRF\pm(t,f;w,\Omega;\theta,\phi)
\]

\end_inset


\end_layout

\begin_layout Standard
r is the response of each Cochleograms convolved in time and frequency by
 a bank of STRF.
 We can see the Corticogram as a repeated Cochleogram viewed at different
 resolution, or as a bank of different spectral and temporal modulation
 filters with different tuning (from narrowband to broadband).
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/cortico1.png
	BoundingBox 0bp 20bp 432bp 288bp

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Corticograms visualization as a matrix of time-frequency representations.
 Each of the plots can be understood as a cochleogram filtered in 2D (rates
 and sclaes).
 Filter frequency are controlled by the scale and the rate parameters.
 In the top left pannel we see a represation with broadband filters in rates
 and scales.
 In bottom right we see a represention with sharpband filter of rate and
 scale, somehow representing the high frequencies of the 'wavelet transform'
 and is closed to the cochleogram.
 The botom right and top left are representation with sharp modulation in
 frequencies and time respectivelly.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The Corticogram is a 4D representation indexed by scale/rate/time/frequency
 matrix.
 As such it can be figured as a set of matrix of Time/Frequency representation.
 interested reader can look a the paper ...
 for better understanding of the computational specificity.
 The implementation here is provided by the NSL toolbox.
\end_layout

\begin_layout Subsection
Quorticograms
\end_layout

\begin_layout Standard
As the CQT offer better performances in computation time and is invertible,
 we simplified the Corticogram by replacing the input Cochleogram by a CQT.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/cortico2.png
	BoundingBox 0bp 20bp 432bp 288bp

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Quorticograms visualization as a matrix of time-frequency representations.
 Each of the plots can be understood as a constant Q transform filtered
 in 2D (rates and scales).
 Filter frequency are controlled by the scale and the rate parameters.
 In the top left panel we see a representation with broadband filters in
 rates and scales.
 In bottom right we see a representation with sharpband filter of rate and
 scale, somehow representing the high frequencies of the 'wavelet transform'
 and is closed to the CQT.
 The bottom right and top left are representation with sharp modulation
 in frequencies and time respectively.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Other Representations
\end_layout

\begin_layout Subsection
SineWave Speech
\end_layout

\begin_layout Standard
The sine wave speech synthesis is obtained by tracking formants (resonant
 frequencies of the vocal track) along time.
 The simplified sound surprisingly reminds of the original.
 This representation is obtained by first finding the formants in the spectrogra
m, then using a Viterbi algorithm for the tracking.
\end_layout

\begin_layout Standard
There is a very good introductory article on sinewave speech on 
\begin_inset CommandInset href
LatexCommand href
name "scholarpedia"
target "http://www.scholarpedia.org/article/Sine-wave_speech"

\end_inset

.
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/sketchified_carglass_SWSSketch.pdf
	width 10cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Spectrogram of a SineWave Speech reconstruction of an audio excerpt.
 red regions are more energetic.
 The signal is sliced in evenly spaced time frames.
 In each frame, the formants (resonant frequencies of the vocal track) are
 searched for.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Feature transforms
\end_layout

\begin_layout Standard
Literature on audio feature computation is proficient.
 Any feature extraction process can be used, (MFCC, chroma, LPC, etc..) any
 feature combination can be thought of as a sketch of the signal.
 Most of the time, the audio features are analysis-oriented and cannot be
 inverted.
 This is for instance the case with Zero-Crossing Rate or spectral moments.
 
\end_layout

\begin_layout Standard
The synthesis can then be obtained by concatenating sounds from a database
 based on nearest neighbors search in the feature space.
 This is for example used in 
\begin_inset CommandInset citation
LatexCommand cite
key "Muller2005"

\end_inset

.
\end_layout

\begin_layout Chapter
Measuring similarities : A generalized Fingerprinting approach
\end_layout

\begin_layout Section
Ideas and concepts
\end_layout

\begin_layout Subsection
From sketches to fingerprints
\end_layout

\begin_layout Standard
Sketches are simplification of sounds.
 They can be obtained using any of the representation described in Chapter
 1 by reducing the dimensionality in the transform domain.
 In order to produce a sketch, one must thus choose a combination of a represent
ation (STFT, CQT, Corticogram..) and a dimensionality reduction technique
 (Hard Thresholding, Greedy selection, Principal Component Analysis, etc..).
\end_layout

\begin_layout Standard
How can we tell two sketches apart based on their effectiveness at capturing
 the essence of a sound? In this project we have chosen the well defined
 field of audio retrieval via acoustic fingerprinting.
 Indeed, sketches can be seen as first step in the building of an acoustic
 fingerprint.
 The basic idea behind this part of the project is to study the properties
 of various sketches and their performances on fingerprint tasks?
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center

\lang french
\begin_inset Graphics
	filename /home/manumouss/workspace/audio-sketch/src/reporting/figures/SparseRep2AudioFingerprint.pdf
	width 10cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
From signal to sketches then to fingerprints.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
The evaluation problem
\end_layout

\begin_layout Standard
Audio objects recognition systems aim at the automatic retrieval of a signal
 
\begin_inset Formula $y$
\end_inset

 among a collection of known sounds objects 
\begin_inset Formula $\{y^{(i)}\}$
\end_inset

.
 In practice, such collection can be very large and sounds are complicated
 objects to compare.
 For this retrieval to be effective, the search must be performed efficiently,
 for instance by comparing low-dimensional 
\emph on
proxies
\emph default
 of the objects, or fingerprints.
\end_layout

\begin_layout Standard
An audio fingerprint is a collection of signal-characteristic features that
 present some robustness to distortions and can be efficiently compared
 to others.
 There are two big families of audio fingerprinting systems, the first one
 adopts a bag of features approach.
 A low-dimensional vector of features (eg.
 Chroma, MFCC, etc..) is thus used as the fingerprint.
 It has essentially been proposed by Haitsma 
\begin_inset CommandInset citation
LatexCommand cite
key "Haitsma"

\end_inset

 with binarized Chroma.
 A review of such methods can be found in 
\begin_inset CommandInset citation
LatexCommand cite
key "Cano_review"

\end_inset

 with more recent avatars being based on wavelet transforms 
\begin_inset CommandInset citation
LatexCommand cite
key "Baluja2007"

\end_inset

 or finer frequency models 
\begin_inset CommandInset citation
LatexCommand cite
key "Betser_article,Dupraz_article"

\end_inset

.
\end_layout

\begin_layout Standard
The second family of methods is similar in spirit to some feature extraction
 methods developed in image processing.
 It has first emerged with the work of Wang 
\begin_inset CommandInset citation
LatexCommand cite
key "Wang"

\end_inset

 and is based on the idea of selecting a subset of 
\emph on
keypoints
\emph default
 in a Time-Frequency (TF) representation, pairing them to form 
\emph on
landmarks
\emph default
 and using each of these 
\emph on
landmarks
\emph default
 as an index in a structured database (
\emph on
e.g.

\emph default
 a hash-table or any fast indexing system).
 This approach is at the basis of the well known Shazam service 
\begin_inset CommandInset citation
LatexCommand cite
key "Wang_com"

\end_inset

, but also led to the works of Cotton and Ellis
\begin_inset CommandInset citation
LatexCommand cite
key "Cotton2010"

\end_inset

 and Fenet
\emph on
 et al
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "ismir"

\end_inset

 among others.
 While in his seminal work 
\begin_inset CommandInset citation
LatexCommand cite
key "Wang"

\end_inset

, Wang selected 
\emph on
keypoints
\emph default
 as local maxima in a simple spectrogram, Cotton and Ellis 
\begin_inset CommandInset citation
LatexCommand cite
key "Cotton2010"

\end_inset

 use a greedy algorithm on a multiscale Gabor dictionary and Fenet 
\emph on
et al
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "ismir"

\end_inset

 propose a logarithmic transform instead of windowed Fourier.
\end_layout

\begin_layout Standard
All of these methods share a common formalism, that is conveniently exposed
 using a dictionary-based point of view.
 Given a dictionary 
\begin_inset Formula $\Phi$
\end_inset

, one seeks a combination of 
\begin_inset Formula $k$
\end_inset

 elements of 
\begin_inset Formula $\Phi$
\end_inset

 (labeled 
\emph on
atoms
\emph default
) that can be efficiently used as 
\emph on
keypoints
\emph default
 in a fingerprinting system.
 State of the art methods, mainly proposes different dictionaries (
\emph on
e.g.

\emph default
 Gabor 
\begin_inset CommandInset citation
LatexCommand cite
key "Wang,Wang_com"

\end_inset

, Union of Gabor
\begin_inset CommandInset citation
LatexCommand cite
key "Cotton2010"

\end_inset

, MDCT 
\begin_inset CommandInset citation
LatexCommand cite
key "Moussallam2012c"

\end_inset

, Logarithmic 
\begin_inset CommandInset citation
LatexCommand cite
key "ismir"

\end_inset

..) and selection algorithms (Local Peak Picking 
\begin_inset CommandInset citation
LatexCommand cite
key "Wang,Wang_com,ismir"

\end_inset

, Matching Pursuit 
\begin_inset CommandInset citation
LatexCommand cite
key "Cotton2010,Moussallam2012c"

\end_inset

).
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /home/manumouss/workspace/audio-sketch/src/icassp14_fgpt/figures/All100Atoms_GTZAN_16files_7xMDCT.png
	lyxscale 20
	width 8cm

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
Empirical Time-Frequency positions of the first 100 selected atoms (blue
 dots) and their marginal distributions observed on 600 audio segments of
 5 seconds each, taken from the GTZAN
\begin_inset CommandInset citation
LatexCommand cite
key "Tzanetakis2002"

\end_inset

 dataset.
 Signals are down-sampled to 8KHz.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Empirical-Biais"

\end_inset

 The marginal on frequency is presented in log scale.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /home/manumouss/workspace/audio-sketch/src/icassp14_fgpt/figures/WF_WT_200files_1xMDCT_100k.pdf
	width 8cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Empirical Co-occurrence of Time Frequency atoms observed on the same 600
 segments.
\begin_inset CommandInset label
LatexCommand label
name "fig:Empirical-Covariance"

\end_inset

.
 The empirical bias has been subtracted.
 Darker regions indicate higher co-occurrences.
 The strong diagonal components indicates neighborhood relationships both
 in time and frequency.
 Harmonic correlations can be observed in the frequency matrix.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Architecture
\end_layout

\begin_layout Standard
The overall architecture of a fingerprint system is described in Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fingerprinting-architecture"

\end_inset

.
 The difficulty for studying sketches on this issue is to maintain a fair
 comparison between them by ensuring the HashTable, and the whole query
 system is equivalent.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/schema.png
	lyxscale 25
	width 6cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Fingerprinting architecture 
\begin_inset CommandInset label
LatexCommand label
name "fig:Fingerprinting-architecture"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this project, we went a step further and developed a unified fingerprinting
 framework.
\end_layout

\begin_layout Section
A fingerprinting framework
\end_layout

\begin_layout Subsection
Problem
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $\boldsymbol{y}\in\mathbb{E}^{N}$
\end_inset

 be a 
\begin_inset Formula $N$
\end_inset

 - dimensional discrete signal (
\begin_inset Formula $\boldsymbol{E}=\mathbb{R}$
\end_inset

 or 
\begin_inset Formula $\mathbb{C}$
\end_inset

) and 
\begin_inset Formula $\boldsymbol{\Phi}=\{\phi_{i}\}_{i=1..M}$
\end_inset

 a dictionary of 
\begin_inset Formula $M$
\end_inset

 
\emph on
atoms 
\emph default

\begin_inset Formula $\boldsymbol{\phi}_{i}$
\end_inset

 of same dimension than 
\begin_inset Formula $\boldsymbol{y}$
\end_inset

, one speaks of a 
\emph on
representation
\emph default
 
\begin_inset Formula $\hat{y}$
\end_inset

 of 
\begin_inset Formula $\boldsymbol{y}$
\end_inset

 in 
\begin_inset Formula $\Phi$
\end_inset

 as a linear combinations of the atoms, 
\emph on
i.e.

\emph default
 
\begin_inset Formula $\hat{y}=\sum_{i=1}^{M}\alpha_{i}\phi_{i}$
\end_inset

 where the weights coefficients stacked in an 
\begin_inset Formula $M-$
\end_inset

dimensional vector 
\begin_inset Formula $\boldsymbol{\alpha}$
\end_inset

 now carry the information.
 The nature and quantity of information conveyed by each (or a combination
 of) 
\begin_inset Formula $\alpha_{i}$
\end_inset

 depends on how the dictionary is designed and what
\emph on
 a priori 
\emph default
knowledge on the signal is available.
 
\end_layout

\begin_layout Standard
In an audio fingerprint context, it is interesting to further decompose
 
\begin_inset Formula $\boldsymbol{\alpha}$
\end_inset

 as the element-wise product 
\begin_inset Formula $\boldsymbol{\alpha}=\boldsymbol{x}\odot\boldsymbol{s}$
\end_inset

 where 
\begin_inset Formula $x$
\end_inset

 is real or complex valued and 
\begin_inset Formula $\boldsymbol{s}$
\end_inset

 is called the support and restricted to binary values: 
\begin_inset Formula $s_{i}=$
\end_inset

1 if the atom is selected as a keypoint and zero otherwise.
\end_layout

\begin_layout Standard
In this formalism, limiting the number of keypoints can be straightforwardly
 transcribed as a sparsity constraint on 
\begin_inset Formula $s$
\end_inset

.
 The robustness property is harder to characterize since different types
 of distortions may occur.
 For the sake of clarity, let us consider only the case of additive white
 Gaussian noise.
 The best way to resist such distortion is to select atoms minimizing a
 reconstruction error.
 More generally, most types of robustness can be enforced by constraints
 of 
\emph on
descriptiveness
\emph default
 of the keypoints.
 
\end_layout

\begin_layout Standard
Expressing the discriminative power, however, is more challenging.
 This can be done by using information theoretic metrics in general and
 entropy in particular.
 Audio signals often carry more energy in their low than high-frequencies.
 Corresponding keypoints thus have a higher probability of being selected.
 Intuitively, they provide a less discriminant information on a signal that
 the least probables ones.
 If one is able to fully evaluate the probability distribution of the support
 then one would want to constrain its 
\emph on
entropy
\emph default
 to be the highest possible.
\end_layout

\begin_layout Standard
The problem of finding 
\begin_inset Formula $k$
\end_inset

 keypoints that have maximum descriptive and discriminative potentials can
 thus be stated as:
\begin_inset Formula 
\begin{equation}
\mathcal{P}_{\lambda,k}=\min_{s}\|y-\sum_{i=1}^{M}x_{i}.s_{i}.\phi_{i}\|_{2}-\lambda H_{\Phi}(s)\mbox{ s.t. }\sum_{i=1}^{M}s_{i}=k\label{eq:problem_full}
\end{equation}

\end_inset

where 
\begin_inset Formula $H_{\Phi}(s)$
\end_inset

 is the entropy of the vector 
\begin_inset Formula $s$
\end_inset

 given the dictionary.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/noisy_keys.pdf
	width 12cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Visualization of landmarks built on the STFT for a sound and the same sound
 + noise.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Generalization
\end_layout

\begin_layout Subsection
Structured Sparsity model
\end_layout

\begin_layout Standard
Empirical evidence suggest the sparsity pattern of the support vector in
 time-frequency dictionaries is highly structured.
 We propose to use Boltzmann machines as a model for the distribution of
 
\begin_inset Formula $s$
\end_inset

: 
\begin_inset Formula 
\begin{equation}
p(s)\propto\exp(b^{T}s+s^{T}Ws)
\end{equation}

\end_inset

This distribution has first been proposed in 
\begin_inset CommandInset citation
LatexCommand cite
key "Hinton1986"

\end_inset

.
 It models the interaction in a graph of connected nodes (keypoints in our
 case) using two parameters: a bias 
\begin_inset Formula $b$
\end_inset

 and a connectivity matrix 
\begin_inset Formula $W$
\end_inset

.
 This model has recently appeared in dictionary based processing setups.
 Dremeau 
\emph on
et al
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "Dremeau2012"

\end_inset

 show that it generalizes many structured sparsity models.
 Under this model, we can evaluate the probability of a state using the
 difference of energy at node 
\begin_inset Formula $i$
\end_inset

:
\begin_inset Formula 
\begin{equation}
\Delta E_{i}=\sum_{j}w_{ij}+b_{i}
\end{equation}

\end_inset

and the notion of temperature 
\begin_inset Formula $T$
\end_inset

 that can be dropped out (
\emph on
e.g.

\emph default
 
\begin_inset Formula $T=1$
\end_inset

).
 Fixing the states of all other variables, the probability of node 
\begin_inset Formula $i$
\end_inset

 being turned on (
\emph on
i.e.

\emph default
 keypoint 
\begin_inset Formula $i$
\end_inset

 being selected) writes: 
\begin_inset Formula 
\begin{equation}
p(s_{i}=1|\{s_{j\neq i}\})=\frac{1}{1+\exp\left(-\Delta E_{i}\right)}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Reducing model complexity
\end_layout

\begin_layout Standard
The expressiveness of the Boltzmann machine is essentially captured by the
 
\begin_inset Formula $W$
\end_inset

 matrix which is of size 
\begin_inset Formula $M\times M$
\end_inset

 where 
\begin_inset Formula $M$
\end_inset

 is the number of atoms in the dictionary.
 Clearly, for real scale data, the resulting model complexity will become
 prohibitive.
 Fortunately, the considered dictionaries are further structured.
 Let assume each atom 
\begin_inset Formula $\phi_{i}$
\end_inset

 can be indexed by a unique triplet 
\begin_inset Formula $(f_{i},t_{i},l_{i})\in\mathcal{F}\times\mathcal{T}\times\mathcal{L}$
\end_inset

 of its frequency and time centroids and length.
 A way to drastically reduce the complexity is to assume separability of
 the time and frequency centroids variables.
 Such hypothesis seem reasonable, keypoints frequency localization will
 essentially be linked to other keypoints frequencies and lengths, independently
 of their time position.
 Symmetrically, time localizations may be considered apart from the frequency
 localization.
\end_layout

\begin_layout Standard
In practice, this implies cutting many vertices in the Boltzmann machine,
 or equivalently putting many elements of 
\begin_inset Formula $W$
\end_inset

 to zero.
 The bias can be rewritten as :
\begin_inset Formula 
\begin{equation}
b_{i}=b(f_{i},t_{i},l_{i})=b(f_{i},l_{i})
\end{equation}

\end_inset

since we have seen empirically that keypoints are uniformly located in time.
 Similarly, each element 
\begin_inset Formula $w_{ij}$
\end_inset

 of the 
\begin_inset Formula $W$
\end_inset

 matrix can be expressed as a product:
\begin_inset Formula 
\begin{eqnarray*}
w_{ij} & = & w\left[\left(f_{i},t_{i},l_{i}\right)\left(f_{j},t_{j},l_{j}\right)\right]\\
 & = & w^{F}\left[\left(f_{i},l_{i}\right)\left(f_{j},l_{j}\right)\right]w^{T}\left[\left(t_{i},l_{i}\right)\left(t_{j},l_{j}\right)\right]\\
 & = & w_{ij}^{F}w_{ij}^{T}
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $w_{ij}^{F}\mbox{ and }w_{ij}^{T}$
\end_inset

 are taken in two factoring matrices 
\begin_inset Formula $W_{F}$
\end_inset

 and 
\begin_inset Formula $W_{T}$
\end_inset

.
 We have seen empirical estimators of such bias and 
\begin_inset Formula $W$
\end_inset

 matrices in Figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Empirical-Biais"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Empirical-Covariance"

\end_inset

.
\end_layout

\begin_layout Subsection
Algorithm
\end_layout

\begin_layout Standard
Addressing problem 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:problem_full"

\end_inset

 is a complicated issue.
 Indeed, even with 
\begin_inset Formula $\lambda=0$
\end_inset

, it requires either a relaxation of the sparsity constraint or the use
 of suboptimal greedy algorithms such as MP.
 Given that the hard sparsity constraint is strict in this case, we have
 chosen to modify an MP algorithm by simply changing the atom selection
 rule.
 
\end_layout

\begin_layout Standard
Such algorithm makes a series of local decisions (
\emph on
i.e.

\emph default
 keypoint selection), based only on the knowledge of the previous choices
 (
\emph on
i.e.

\emph default
 which keypoints have already been selected).
 The residual signal 
\begin_inset Formula $r^{n}$
\end_inset

 at iteration 
\begin_inset Formula $n$
\end_inset

 is usually updated by subtracting from the original signal its projection
 on the subspace spanned by the selected atoms.
 At iteration 
\begin_inset Formula $n$
\end_inset

 the decision boils down to solving:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\arg\max_{\phi_{i}\in\Phi}|\langle r^{n},\phi_{i}\rangle|(1+\lambda_{H}H(\phi_{i}|s_{n-1}))
\end{equation}

\end_inset

where 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $H(\phi_{i}|s_{n-1})$
\end_inset

 is the
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 entropy of choosing atom 
\begin_inset Formula $\phi_{i}$
\end_inset

 knowing the support 
\begin_inset Formula $s_{n-1}$
\end_inset

 and writes: 
\begin_inset Formula 
\begin{eqnarray}
H(\phi_{i}|s_{n-1}) & = & -p(\phi_{i}|s_{n-1})\log\left[p(\phi_{i}|s_{n-1})\right]\nonumber \\
 & = & \frac{\log\left[1+\sum_{j\in\Gamma_{n-1}}w_{ij}+b_{i}\right]}{1+\sum_{j\in\Gamma_{n-1}}w_{ij}+b_{i}}
\end{eqnarray}

\end_inset

with 
\begin_inset Formula $\Gamma_{n-1}$
\end_inset

 being the indices of the non zero elements of 
\begin_inset Formula $s_{n-1}$
\end_inset

, 
\emph on
i.e.

\emph default
 the keypoints selected so far.
 An advantage of this algorithm is that it can be quickly implemented using
 existing MP libraries such as PyMP
\begin_inset Foot
status open

\begin_layout Plain Layout
https://github.com/mmoussallam/PyMP
\end_layout

\end_inset

.
 Additionally, existing algorithms can be seen as particular cases.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/KeyPoints_and_pairs_voicefemale_30k.pdf
	width 16cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Time-frequency 
\emph on
landmarks
\emph default
 built by the algorithm with varying parameters on a 5s audio excerpt of
 female speech.
 Each case has built 100 landmarks.
 (a): C10 (
\begin_inset Formula $\lambda_{H}=0$
\end_inset

) (b): 
\begin_inset Formula $\lambda_{H}=1\,(0,W)$
\end_inset

 (c): 
\begin_inset Formula $\lambda_{H}=10\,(b,W)$
\end_inset

(d): W03
\begin_inset CommandInset label
LatexCommand label
name "fig:TF-landmarks"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Observations
\end_layout

\begin_layout Standard
In the framework described above, many parameters need to be chosen.
 The dictionary 
\begin_inset Formula $\Phi$
\end_inset

, the sparsity 
\begin_inset Formula $k$
\end_inset

 of the representation (
\emph on
i.e.

\emph default
 the number of keypoints), 
\begin_inset Formula $\lambda_{H}$
\end_inset

, the bias 
\begin_inset Formula $b$
\end_inset

 and the 
\begin_inset Formula $W$
\end_inset

 matrix.
 We will adopt the notation W03 and C10 by reference to 
\begin_inset CommandInset citation
LatexCommand cite
key "Wang"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "Cotton2010"

\end_inset

 respectively.
 Note however that all results presented here are obtained with our own
 implementation of these methods.
 W03 corresponds to a local peak picking strategy with a monoscale Gabor
 dictionary.
 C10 is equivalent to our algorithm with 
\begin_inset Formula $\lambda_{H}$
\end_inset

 being set to 
\begin_inset Formula $0$
\end_inset

.
 We investigate hybrid strategies with a simple synthetic frequency bias
 
\begin_inset Formula $b$
\end_inset

 and a neighbor penalizing matrix 
\begin_inset Formula $W$
\end_inset

.
\end_layout

\begin_layout Standard
Experiments are run in a framework that is similar in nature to the one
 presented in 
\begin_inset CommandInset citation
LatexCommand cite
key "Wang,ismir,Cotton2010"

\end_inset

.
 Landmarks are binarized and stored as index keys in a Hashtable implemented
 using the open source Berkeley DataBase C Library
\begin_inset Foot
status open

\begin_layout Plain Layout
http://www.oracle.com/us/products/database/berkeley-db/overview/index.html
\end_layout

\end_inset

.
 Each key is a combination 
\begin_inset Formula $(f_{1},f_{2},\Delta t)$
\end_inset

 where 
\begin_inset Formula $f_{1}$
\end_inset

 and 
\begin_inset Formula $f_{2}$
\end_inset

 are the frequency centroids of the two keypoints and 
\begin_inset Formula $\Delta_{t}$
\end_inset

 the difference between their time centroids.
 At each key corresponds a value that is a combination of the file index
 and the time of occurrence of the landmark in the file.
 In this work we are interested in comparing the keypoints and landmarks
 selection procedures.
 For fair comparison, the hashing and key formatting parameters are not
 optimized to any of the methods but fixed to common values.
 
\end_layout

\begin_layout Standard
In the following, a set of parameters will be identified by the triplet
 
\begin_inset Formula $(\lambda_{H},b,W)$
\end_inset

.
 Whenever the bias (respectively 
\begin_inset Formula $W$
\end_inset

) is set to zero we will write 
\begin_inset Formula $\lambda_{H}\,(0,W)$
\end_inset

 (respectively 
\begin_inset Formula $\lambda_{H}\,(b,0)$
\end_inset

).
 In this work we use a synthetic bias that is a simple decreasing exponential
 of the keypoints frequencies.
 
\begin_inset Formula $W$
\end_inset

 is decomposed in 
\begin_inset Formula $W_{T}$
\end_inset

 and 
\begin_inset Formula $W_{F}$
\end_inset

 that are zeros everywhere except near the diagonals.
 This particular setting corresponds to penalizing the selection of keypoints
 in the neighborhood of previously selected ones.
 For now we do not penalize harmonic relationships.
 Non zero coefficients in 
\begin_inset Formula $W_{T}$
\end_inset

 and 
\begin_inset Formula $W_{F}$
\end_inset

 are adapted to the desired sparsity level and corresponds to the same Time-Freq
uency widths as the ones used in W03 for local peak picking.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="5">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Parameter
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
W03
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
C10
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Mixed-I
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Mixed-II
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\Phi$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
monoscale
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
multiscale
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
multiscale
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
multiscale
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\lambda_{H}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\infty$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $0$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\in\mathbb{R}^{+}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\in\mathbb{R}^{+}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $b$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
no
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
no
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
fixed
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
learned
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $W$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
implicit 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
no
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
fixed
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
learned
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Fingerprint parameters
\begin_inset CommandInset label
LatexCommand label
name "tab:Fingerprint-parameters"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Keypoints and Landmarks entropy
\end_layout

\begin_layout Standard
We expect keypoints selected with the entropic penalization to have a distributi
on somehow more 
\begin_inset Quotes sld
\end_inset

uniform
\begin_inset Quotes srd
\end_inset

 that those selected on purely energetic considerations.
 Indeed Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Empirical-Entropy"

\end_inset

 shows empirical distributions measured on the decomposition of 600 random
 5 second length segments taken from the GTZAN
\begin_inset CommandInset citation
LatexCommand cite
key "Tzanetakis2002"

\end_inset

 dataset.
 Keypoints and Landmarks selected with the W03 method are almost uniformly
 distributed, while the ones built by C10 exhibit a strong bias towards
 low frequencies.
 The hybrid approach allows to reach a new compromise.
 
\end_layout

\begin_layout Standard
An illustration of this behavior is provided in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:TF-landmarks"

\end_inset

.
 For 4 different settings, 100 landmarks are built and figured with black
 segments.
 Recall that C10 is similar to 
\begin_inset CommandInset citation
LatexCommand cite
key "Cotton2010"

\end_inset

.
 The algorithm selected atoms on a purely energetic basis in a union of
 6 scales Gabor shift-invariant dictionary.
 A first hybrid approach (labeled 
\begin_inset Formula $\lambda_{H}=5\,(0,W)$
\end_inset

) uses the same dictionary with 
\begin_inset Formula $\lambda_{H}=1$
\end_inset

 the bias is set to zero and 
\begin_inset Formula $W$
\end_inset

 as described above.
 The penalization led to the selection of a slightly different set of keypoints.
 A second hybrid approach (labeled 
\begin_inset Formula $\lambda_{H}=10\,(b,W)$
\end_inset

) uses both 
\begin_inset Formula $W$
\end_inset

 and the bias 
\begin_inset Formula $b$
\end_inset

 to penalize the selection.
 This time the algorithm has selected a very different set of keypoints.
 Finally, the last case (W03) use a monoscale Gabor dictionary with 25%
 overlap and a selection procedure equivalent to 
\begin_inset CommandInset citation
LatexCommand cite
key "Wang"

\end_inset

.
 This is similar to what could be obtained with no bias, the same 
\begin_inset Formula $W$
\end_inset

 and 
\begin_inset Formula $\lambda_{H}=+\infty$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /home/manumouss/workspace/audio-sketch/src/icassp14_fgpt/figures/EmpiricalKPLMdistribs_GTZAN_600_60k_6xLOMDCT_50bins.pdf
	width 8cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Empirical distribution of landmarks.
 Flatter distribution has higher entropy and corresponds to cases where
 each landmark is more discriminant.
 Landmarks are indexed by increasing frequencies of the first keypoint.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Empirical-Entropy"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Robustness and Recognition performances
\end_layout

\begin_layout Standard
The primary objective of a fingerprint system is its recognition performance.
 The main parameter affecting the quality of the results is the sparsity,
 or equivalently the number of landmarks per seconds on which to base the
 decision.
 For each query, the system returns a best candidate file in the database
 and an estimated time of occurrence.
 The score is simply the ratio of the number of correctly retrieved segments
 over the number of queries.
 To assess for the robustness of the fingerprints, we measure the Proportion
 of Identical Landmarks (PIL) that remain unaffected by an additive white
 Gaussian noise.
\end_layout

\begin_layout Standard
Using different parameters a learning an a testing phase are run.
 During the learning phase, a database of fingerprints is built out of the
 1000 files of the GTZAN dataset
\begin_inset CommandInset citation
LatexCommand cite
key "Tzanetakis2002"

\end_inset

.
 Each file is sliced in 5 seconds-long segments with a 50% overlap.
 On each segment the number of allowed keypoints is fixed to the sparsity
 level 
\begin_inset Formula $k$
\end_inset

.
 During the test phase, 2500 randomly chosen 5 seconds-long segments are
 used for queries.
 The testing segments thus have a very low probability of being aligned
 with the learning ones.
\end_layout

\begin_layout Standard
The compromise between the two concurrent objectives is illustrated on Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Recognition-Robustness"

\end_inset

.
 C10 is the more robust method but performs poorly on the recognition task
 at low levels of sparsities.
 On the opposite, W03 reaches very good recognition scores at low sparsities,
 but is also the more affected by the additive noise.
 Between them, the hybrid approaches allows to reach different compromises.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /home/manumouss/workspace/git/audio-sketch/src/manu_sandbox/figures/schema.png
	lyxscale 25
	width 4cm

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
Fingerprinting scheme common to 
\begin_inset CommandInset citation
LatexCommand cite
key "Wang,ismir,Moussallam2012c,Cotton2010"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /home/manumouss/workspace/audio-sketch/src/icassp14_fgpt/figures/robustness_7snrs_20segs_2tests_30sparsity.pdf
	width 8cm

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /home/manumouss/workspace/audio-sketch/src/icassp14_fgpt/figures/reco_complete_GTZAN_1000.pdf
	width 8cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Top: Robustness results with synthetic 
\begin_inset Formula $W$
\end_inset

 and varying 
\begin_inset Formula $\lambda_{H}$
\end_inset

 averaged over 5 trials of random Gaussian noise on each of 600 random segments
 of 5 seconds taken in the GTZAN dataset.
 Bottom: Recognition performances for isolated 5sec audio excerpts from
 the complete dataset with various settings, function of the number of keypoints
 
\begin_inset Formula $k$
\end_inset

.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Recognition-Robustness"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Recent experiments
\end_layout

\begin_layout Standard
In chapter 2 we have seen a generalization of the Shazam fingerprinting
 framework as a sparse decomposition problem.
 In this chapter we will now focus on the other types of sketches that we
 are interested in in this project.
\end_layout

\begin_layout Section
Sketches performances on music recognition
\end_layout

\begin_layout Standard
The first test we need to run is to evaluate the capacity of Cochleograms
 and CQT representation to capture a signature as efficiently as a STFT
 representation.
 We tested the recognition rate of three different sounds representations:
 STFT, CQT and Cochleograms in a musical database using fingerprint methods.
 The STFT approach is here equivalent to the Shazam approach or the W03
 method presented in chapter 2.
 We do not show results for C10 or hybrid approaches for the sake of clarity.
 Results are summarized in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Results-CQT-STFT"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/results_Laure_CQT.pdf
	width 16cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\lang french
Results on identifcation task over tree differents models of sound representatio
n (STFT, CQT, Cochleogram), on the left: recognition rate of the tree representa
tions over the added white noise, on the rigth: computation time over the
 size of the database.
\begin_inset CommandInset label
LatexCommand label
name "fig:Results-CQT-STFT"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Conclusion: Auditory-based representations seems just as capable of handling
 fingerprints as the STFT is.
 
\end_layout

\begin_layout Itemize
The CQT representation outperforms the Cochleogram on recognition rate and
 computation time.
 This means the CQT is a viable alternative to the use of Cochleogram.
 Both representation use the same frequency range with the same resolution,
 but the perfect invertibility (under the overlapp-add conditions) of the
 CQT seems to make a significant difference.
 
\end_layout

\begin_layout Itemize
The STFT still perform better results as the range of frequencies is wider.
 Note however, that although we do not present results here, CQT peaks exhibit
 robustness to a much wider range of distortion than STFT.
\end_layout

\begin_layout Section
Sketches performances on speech
\end_layout

\begin_layout Standard
We are now interested in characterizing the performances of the various
 sketches on speech recordings.
 We used the Voxforge database for these experiments.
 This dataset is composed of 8 english speakers uttering the same 1130 sentences.
 Speakers are 5 men and 3 women, with various accent ranging from indian
 to texan and british.
 
\end_layout

\begin_layout Standard
This dataset is very interesting because it contains a lot of diversity
 along with some redundancies that we try to discover in this chapter.
\end_layout

\begin_layout Subsection
Exact recognition on speech databases
\end_layout

\begin_layout Standard
The problem of finding an excerpt among all the existing one is here exactly
 the same as above for musical databases.
 Sound objects are quite distinguishable and have various lengths (ranging
 from 1.5 to 9 seconds).
 The task of finding an element based on its fingerprint can thus be achieved
 with any type of sketch provided enough elements are taken.
\end_layout

\begin_layout Standard
What is interesting though, is to compare the recognition rate at the specific
 sparsity value that is sufficient for a plain STFT Pick picking to achieve
 100% recognition rate.
 That is, for different sketches, we evaluate the recognition rate at this
 sparsity:
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="13" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Sketch
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Recognition rate (maximum sparsity)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
STFT Peak Picking
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
100%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
STFT Thresholding
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
80%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Corticogram Peak Picking
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Corticogram IHT
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
10%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Quorticogram Peak Picking
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Quorticogram IHT
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
10%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Cohleogram Peak Picking
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
40%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Cohleogram IHT
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
20%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Sparse xMDCT
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
80%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
CQT Peak Picking
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
80%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
CQT IHT
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
40%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
SineWave Speech
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0%
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
Recognition rate at very low sparsity (lowest sufficient for 100% recognition
 rate with STFT Peak Picking)
\begin_inset CommandInset label
LatexCommand label
name "tab:Recognition-exact"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Recognition-exact"

\end_inset

 must be read in the following manner: the STFT PP being a reference, IHT
 methods seem to perform rather poorly at very low sparsity (though they
 exhibit the highest robustness to distortions).
 Hybrid approach such as developed in Chapter 2 and CQT based Peak Picking
 behave quite correctly.
 Representation of higher dimensions (Cortico and Quorticogram) are not
 well suited to this task, the sparsity level is just too low.
 Listening to these reconstructions, one is clearly not able to recognize
 the original excerpt.
\end_layout

\begin_layout Standard
SineWave Speech is a very odd case in this setup.
 Although the perceived similarity is quite high, classical fingerprinting
 systems are completely unable to recognize the object from its SineWave
 Speech.
 We have Tried many different parameter combinations (number of formants,
 time step, formant smoothing, etc..) but none prove to be efficient on this
 task.
\end_layout

\begin_layout Subsection
Finding the same sentence with a different speaker
\end_layout

\begin_layout Standard
Exact recognition is a somewhat easy problem to solve.
 On the opposite, we are now interested in measuring how fingerprints based
 on sketches may allow to recognize a sentence when it is pronounced by
 a different speaker.
 For this task we use 10 sentences and we try to guess which has been uttered
 based on a fingerprint database learned on a different speaker.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="7" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Sketch
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Recognition rate (different speaker)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
STFT Peak Picking
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
STFT Thresholding
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
10%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Cohleogram IHT
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
20%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Sparse xMDCT
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
10%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
CQT Peak Picking
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
20%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
CQT IHT
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
20%
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
Recognition rate when learning the fingerprints on a different speaker.
\begin_inset CommandInset label
LatexCommand label
name "tab:Recognition-diff"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Recognition-diff"

\end_inset

 is voluntarily pessimistic and is here to emphasize the difficulty of what
 we are now trying to do.
 Classical fingerprints are not suited at all for the task of recognizing
 a sentence uttered by a different speaker.
 The problem is to find invariants in the sketches, which is not what fingerprin
ts do.
\end_layout

\begin_layout Section
Invariant time Keys
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/Compare___same_11_CQTPeaksTripletsBDB.pdf
	width 14cm

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/Compare___same_11_SparseFramePairsBDB.pdf
	width 14cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Landmarks (in yellow) for the same sentence uttered by two speakers.
 The common ones are figured in black
\begin_inset CommandInset label
LatexCommand label
name "fig:Landmarks-diff"

\end_inset

.
 Top: using Triplets of CQT keypoints as landmarks.
 Bottom: using the C10 algorithm.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The problem is illustrated on Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Landmarks-diff"

\end_inset

 two sounds that contain a similar acoustic information (here the same sequence
 of words) but highly non-linear and random distortions (here being pronounced
 by two speakers).
 What are the invariants between those two sounds? And how can they be formalize
d as fingerprints.
 we show her two attempts that we have developed:
\end_layout

\begin_layout Subsection
Triplets of time-frequency points
\end_layout

\begin_layout Standard
The idea here was to use three keypoints to build landmarks described by:
\begin_inset Formula 
\[
\left[f_{1},\Delta f_{1},\Delta f_{2},\frac{\Delta T_{2}}{\Delta T_{1}}\right]
\]

\end_inset

where the 
\begin_inset Formula $\Delta f$
\end_inset

s are the frequency differences and the ratio of the time differences is
 intended to be robust to large time variations.
 In order to seek for maximum invariability we have used the CQT to select
 the keypoints.
 The top of Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Landmarks-diff"

\end_inset

 present the landmarks build with this model.
 Unfortunately, as one can see, the invariant are not sufficient and very
 few landmarks remain common to both fingerprints.
\end_layout

\begin_layout Standard
Although our results so far have been disappointing, we believe there are
 still some developments one should try over this method.
 First, we lack a formalism such as the one presented in Chapter 2, second,
 some heuristic rules could help select among the keypoints the most informative
 ones.
\end_layout

\begin_layout Subsection
Reconstruction-oriented landmarks
\end_layout

\begin_layout Standard
Contrary to what could be thought, reconstruction oriented landmarks such
 as the ones depicted at the bottom of Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Landmarks-diff"

\end_inset

, are among the best performing ones that we have found.
 However, many of the common landmarks that we see should be discarded as
 false positive.
 In fact, we think these fingerprints should perform reasonably well on
 an adjacent task of speaker recognition since the keys seem to have some
 kind of speaker specificity.
\end_layout

\begin_layout Section
Finding invariants in 4 dimensional objects
\end_layout

\begin_layout Standard
Our main hope on this task was to observe invariant in the scale-space domain.
 Indeed, the kind of regularity that we seek can be thought of as scaling
 and shifting in a Time-frequency domain which should be straightforwardly
 seen in a scale-space representation such as Corticograms.
\end_layout

\begin_layout Standard
Our first idea was to simply look at extrema localizations differences between
 sketches of varying speaker and/or sentence.
 We distinguish two types of extrema: 
\end_layout

\begin_layout Enumerate
Global: The 100 first magnitude maximums on each scale/rate.
\end_layout

\begin_layout Enumerate
Local: The Peak Picked maximums, as used in the Shazam approach.
\end_layout

\begin_layout Paragraph
Looking for maximums
\end_layout

\begin_layout Standard
The experiment was made over 6 speakers (4 male and 2 female), for 6 different
 sentences, on Corticogram and Cochleograms for 30 scales/rates parameters,
 for normal and logarithmic representation.
 In this work we only show 3 different scale/rate on the Quorticogram representa
tion for the PP algorithm and one for the global maximum.
 Results are presented in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Multi_quortico"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/Quortico/Locuteurs/Locuteurs0plot0_pp100.pdf
	scale 25

\end_inset


\begin_inset Graphics
	filename figures/Quortico/Locuteurs/Locuteurs1plot0_pp100.pdf
	scale 25

\end_inset


\begin_inset Graphics
	filename figures/Quortico/Phrases/Phrases0plot0_pp100.pdf
	scale 25

\end_inset


\begin_inset Graphics
	filename figures/Quortico/Phrases/Phrases0plot0_pp100.pdf
	scale 25

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/Quortico/Locuteurs/Locuteurs0plot14_pp100.pdf
	scale 25

\end_inset


\begin_inset Graphics
	filename figures/Quortico/Locuteurs/Locuteurs1plot14_pp100.pdf
	scale 25

\end_inset


\begin_inset Graphics
	filename figures/Quortico/Phrases/Phrases0plot14_pp100.pdf
	scale 25

\end_inset


\begin_inset Graphics
	filename figures/Quortico/Phrases/Phrases1plot14_pp100.pdf
	scale 25

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/Quortico/Locuteurs/Locuteurs0plot29_pp100.pdf
	scale 25

\end_inset


\begin_inset Graphics
	filename figures/Quortico/Locuteurs/Locuteurs1plot29_pp100.pdf
	scale 25

\end_inset


\begin_inset Graphics
	filename figures/Quortico/Phrases/Phrases0plot29_pp100.pdf
	scale 25

\end_inset


\begin_inset Graphics
	filename figures/Quortico/Phrases/Phrases1plot29_pp100.pdf
	scale 25

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/Quortico/Locuteurs/Locuteurs0plot14_dump100.pdf
	scale 25

\end_inset


\begin_inset Graphics
	filename figures/Quortico/Locuteurs/Locuteurs1plot14_dump100.pdf
	scale 25

\end_inset


\begin_inset Graphics
	filename figures/Quortico/Phrases/Phrases0plot14_dump100.pdf
	scale 25

\end_inset


\begin_inset Graphics
	filename figures/Quortico/Phrases/Phrases1plot14_dump100.pdf
	scale 25

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
In the first tree top rows the representation of local maximums in a broad,
 middle and sharp scale/rate of the quorticogram.
 In the bottom row the representation of global maximum in a middle scale/rate
 of the quorticogram.
 The two left representations are for the same sentence uttered by two different
 speakers, the two right ones, the representations of two different sentences
 for the same speaker.
\begin_inset CommandInset label
LatexCommand label
name "fig:Multi_quortico"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Conclusion: There are some visual similarities but patterns exhibit a large
 variance.
 
\end_layout

\begin_layout Itemize
In a future experiment it could be interesting to reduce all the parasite
 variability we know in order to focus on the one we want to characterize.
 We can reduce the time variability, by doing a DTW algorithm, or by taking
 shorter excerpts of sound such as vowels interjection.
\end_layout

\begin_layout Itemize
Taking fixed number of maximums over different scale, does not match the
 human processing of sound as theses different layers bring a weighted contribut
ion.
 Comparing the indexed 4 dimensional maximums (scale/rate/frequency/time)
 extract on a complete weighted representations would be more informative.
\end_layout

\begin_layout Section
Sine-Wave speech
\end_layout

\begin_layout Standard
Throughout our experiments, SineWave Speech constitutes a very odd sketch.
 Despite numerous attempts over the past year, we have struggled to match
 its obvious psycho-acoustical properties to our recognition metrics.
 We present here some insights as to why it may have been so and some of
 the leads we have to overcome this.
\end_layout

\begin_layout Subsection
The problem of time
\end_layout

\begin_layout Standard
The problem we found in comparing Sine Wave speech representation, is the
 time variation of each phonemes between different speakers.
\end_layout

\begin_layout Standard
One way of performing a time alignment between two different time series,
 widely used by the community of speak recognizers is the algorithm of Dynamic
 Time Wrapping.
 This works is doing by matching the two series in a space showing the similarit
y of them frames by frames.
 Such a representation is illustrating in fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:DTWwork"

\end_inset

.
 Then a dynamic path is determined to maximize the alignment.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/Sim_sws_dtw.pdf
	scale 50
	BoundingBox -130bp 230bp 595bp 600bp

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Output of the similarity matrix, darker regions shows the maximum similarity
 magnitude.
 The red path is the optimised solution to the alignement of the two time
 series.
 Here the CQT representation of the Sine wave speech of the same sentence
 uttered by two differnet speakers.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:DTWwork"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The path is computed with an implementation of a simple dynamic programming
 algorithm that allows three steps - (1,1), (0,1) and (1,0) - with equal
 weights.
 The implementation is from the Laboratory for the Recognition and Organization
 of Speech and Audio - LabROSA 
\begin_inset Foot
status open

\begin_layout Plain Layout
Download at http://labrosa.ee.columbia.edu/matlab/dtw/
\end_layout

\end_inset


\end_layout

\begin_layout Standard
To better show the effect on DTW algorithm we applied it on two CQT representati
ons of a sine Wave speech of the same sentence uttered by two different
 speakers.
 The results are shown below 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:CQTDTW_comp"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/CQT_sws_DTW.pdf
	scale 50
	BoundingBox -130bp 195bp 543bp 590bp

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
CQT represention of the sine-wave output (3 formants) of two identic sentences
 uterred by two diffrents speakers, respectively 1 and 2.
 The bottom figure show the result of the Dynamic Time Wrapping of sequence
 2 over the senquence 1.
\begin_inset CommandInset label
LatexCommand label
name "fig:CQTDTW_comp"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Even if the two speakers have high temporal dissimilarities, speaker 2 is
 slower than speaker 1 and blurred noise was present at the end of the recording.
 The algorithm find the good features to proceed the alignment.
\end_layout

\begin_layout Subsection
Using MPCA for classification
\end_layout

\begin_layout Subsubsection
From PCA to MPCA
\end_layout

\begin_layout Standard
The Principal Component analysis is an unsupervised technique of dimensional
 reduction of a dataset, represented as a matrix 
\begin_inset Formula $X$
\end_inset

, of size 
\begin_inset Formula $m\times n$
\end_inset

, where 
\begin_inset Formula $n$
\end_inset

 is a number of variables and 
\begin_inset Formula $m$
\end_inset

 the number of repetition of each variables.
 In order to do so the PCA reduce as much as possible the interrelated variables
 of the data set, and retain the variation present in the original data
 set.
 Interested reader could look at 
\begin_inset CommandInset citation
LatexCommand cite
key "Shlens2009"

\end_inset

 for intuitive explanations.
\end_layout

\begin_layout Itemize
Two variables 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 are strongly interrelated if the magnitude of the inter-correlation coefficient
s 
\begin_inset Formula $\sigma_{AB}$
\end_inset

 is important, and the variability of each variable 
\begin_inset Formula $A$
\end_inset

 is set by the autocorrelation coefficient 
\begin_inset Formula $\sigma_{A}^{2}$
\end_inset

.
 Then PCA re-express the data 
\begin_inset Formula $X$
\end_inset

 with a new basis 
\begin_inset Formula $P$
\end_inset

: 
\begin_inset Formula $Y=PX$
\end_inset

, where the correlation matrix 
\begin_inset Formula $C_{Y}=YY^{T}$
\end_inset

is 
\shape italic
diagonal.
\end_layout

\begin_layout Itemize
The dimension reduction is made by preserving only the first components
 of 
\begin_inset Formula $P$
\end_inset

 (witch are the so-called 
\shape italic
principal component
\shape default
).
\end_layout

\begin_layout Standard
The Multiple Principal Component Analysis as the similar function but for
 analyzing tensor structure data (2D or 3D)
\shape italic
.
 
\shape default
Their properties over (2D) PCA analysis are discused in 
\begin_inset CommandInset citation
LatexCommand cite
key "Hung2012"

\end_inset

.
 The algorithm used in the next subsection is detail in 
\begin_inset CommandInset citation
LatexCommand cite
key "Lu2008"

\end_inset

, and is available on the MATLAB file exchange platform 
\begin_inset Foot
status open

\begin_layout Plain Layout
Download at http://www.mathworks.com/matlabcentral/fileexchange/ 
\end_layout

\end_inset

.
\end_layout

\begin_layout Subsubsection
Experiment on sine-wave speech
\end_layout

\begin_layout Standard
In order to evaluate the impact of DTW on sine wave speech, and to know
 better about the similarity inner sine-wave speech, we did two experiments
 on a database sine-wave speech representation (3 formant) of 6 different
 sentences each uttered by 3 different speakers.
 
\end_layout

\begin_layout Enumerate
Experiment 1: Practice MPCA analysis of the database, sine wave speech are
 shortened to their first 135 frames.
\end_layout

\begin_layout Enumerate
Experiment 2: Practice MPCA analysis of the data base, for each sentence,
 the longer and shorter utterance have been time wrapped on the third one,
 we kept the first 135 frames.
\end_layout

\begin_layout Standard
The output of the MPCA is the representation of the data 
\begin_inset Formula $X$
\end_inset

 into the principal component space.
 As the MPCA keep the tensorial form of the input, we take the vectorization
 of the tensorial features of the output.
 To shows the similarity between each individual cases, we represent in
 fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Multidimensional-scalling-(MDS)"

\end_inset

 the distance present between cases of the data set in 2 dimensional space
 by using the multidimensional scaling (MDS).
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/MDS_sws_sh.pdf
	scale 40
	BoundingBox 46bp 193bp 556bp 590bp

\end_inset


\begin_inset Graphics
	filename figures/MDS_sws_DTW.pdf
	scale 40
	BoundingBox 66bp 193bp 558bp 590bp

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Multidimensional scaling (MDS) applied on similarity of the results of the
 MPCA analysis.
 On the left the 2 dimensional representation of the shortened set of sine-wave
 speech, on the right, the representation on the time wrapped and shortened
 set of sine wave speech.
 Colors show the different speakers.
\begin_inset CommandInset label
LatexCommand label
name "fig:Multidimensional-scalling-(MDS)"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
A standard non-metric MDS was performed (matlab, the MathWorks) and lead
 to the next conclusions:
\end_layout

\begin_layout Paragraph
Conclusion: The DTW applied on sine-wave speech show than the variability
 of excerpt is speakers relative.
 
\end_layout

\begin_layout Itemize
The results shown on the figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Multidimensional-scalling-(MDS)"

\end_inset

, have to be read carefully.
 The fidelity of the MDS representation depends of the dimension reduction
 permit by the input data cases.
 In the case of dynamic time wrapped element, the two first component carry
 among the weight of 70% of the total variance, then the representation
 is reliable enough and we can conclude that the dissimilarity into excerpt
 of the data is more speakers relative, than sentences relative.
\end_layout

\begin_layout Itemize
In the case of simple shortened excerpt the two first component carry only
 40% of the total weight, we only conclude of the noisiness of the data.
\end_layout

\begin_layout Itemize
In an other experiment we found than the first two component of the MPCA
 of time wrapped sine-wave speech of the third formant only, carry around
 80% of the variability.
 For the second formant the variability explained is 60%, and the first
 44%.
\end_layout

\begin_layout Section
From psycho-physical experiments to computational simulations (and back)
\end_layout

\begin_layout Subsection
psycho-physical experiments
\end_layout

\begin_layout Standard
In the paper 
\begin_inset CommandInset citation
LatexCommand cite
key "Suied2013"

\end_inset

 Suied & Drémeau 2013 perform two different experiments to test the ability
 of listener to recognize sketches.
 The first experiment test the overall recognition process of human listener,
 using sketches from difference psychoacoustics models of the sound (Cochleogram
/Corticogram).
 The second experience evaluate the processing of sketches, comparing the
 understanding of human listener for two different sparse method (IHT/PP).
\end_layout

\begin_layout Enumerate
The IHT (Iterative Hard Thresholding) algorithm select iteratively a set
 of sparse features, by doing a threshold.
 The stop criteria is the precision of reconstruction.
\end_layout

\begin_layout Enumerate
The PP (Peaks Picking) algorithm choose the local extreme values.
\end_layout

\begin_layout Standard
The data base consisted of the recorded 4 nonverbal emotion (anger, disgust,
 happiness and sadness) of the interjection (on the french vowel 
\begin_inset Formula $/a/$
\end_inset

) produced by 10 different actors.
 The results are illustrated by the two figure below.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/FigureSuiedDremeau1.png
	width 12cm
	BoundingBox 0bp 0bp 886bp 573bp

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Results of the experiment 1: Recognition performance of the sketches sounds
 corresponding to two different auditory models.
 With an without noise the auditory spectogram (cochleogram) outperform
 the score of the corticogram.
 The left plot show the results with a reconstructed clear signal over the
 right plot who show a noisy reconstructed signal.
 Two limits are sets: the lower one is the chance level (dotted line) and
 the upper one (black line) is the average recognition of original sound.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The first experiment show that the recognition on Cochleograms sketches
 outperform the one in Corticogram sketches.
 This is surprising as the Corticogram carry more information, and according
 to the study made in [timbre] this additional information is consequent
 for algorithm in classification of musical timbre.
 There are two hypothesis on this:
\end_layout

\begin_layout Enumerate
The sounds are too shorts (1s) to see the effects of the modulations filters.
 
\end_layout

\begin_layout Enumerate
This information help algorithm by mimicking the brain understanding of
 sound, but can be redundant for the human listeners, then the sparse reconstruc
tion of sounds loose features in layers witch do not help the auditory recogniti
on of a sound.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/FigureSuiedDremeau2.pdf
	BoundingBox 0bp 570bp 595bp 760bp

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Results of the experiment 2: Recognition performance of the sketches sounds
 (only the Cochleogram) corresponding to two differents sparse models.
 The PP algorithm outcast the performance of the IHT (Iterative Hard Thresholdin
g) algorithm, even with noise, but not for noised signal witch 1000 sparses
 features/second.
 Two limits are set: the lower one is the chance level (dotted line) and
 the upper one (black line) is the average recognition of original sound.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The second experiment show that the PP algorithm outperforms the IHT algorithms.
 This can be due to the fact that even if IHT performs a better reconstruction
 of the sound it cover a smaller range of features scale that PP algorithm.
\end_layout

\begin_layout Standard
In both experiences we see a surprising change of recognition results in
 signal with noise reconstructed with 1000 features.
 in the both experiment the explanations are:
\end_layout

\begin_layout Enumerate
Cochleogram/Corticogram: Corticogram excluded noise in layers with the non-impor
tant informations (could be also use for de-noising task) then outperform
 Cochleograms in noise.
\end_layout

\begin_layout Enumerate
IHT/PP: IHT, by selecting better features to reconstruction, excluded noise
 witch is not the case of PP procedure selecting local maxima.
\end_layout

\begin_layout Subsection
Computational simulations
\end_layout

\begin_layout Standard
We did the same as experiment 2, but instead of listeners we test the ability
 of fingerprints in recognition (key overlap ratio) on CQT sketches:
\end_layout

\begin_layout Enumerate
Characterized the recognition process of fingerprints on sketches.
\end_layout

\begin_layout Enumerate
Correlate the result of human psychoacoustic perception with a fingerprinting
 measure.
\end_layout

\begin_layout Standard
The results are shown below:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/SparsityRobustness_k-5bar.png
	scale 50
	BoundingBox 0bp 0bp 432bp 280bp

\end_inset


\begin_inset Graphics
	filename figures/SparsityRobustness_k3bar.png
	scale 50
	BoundingBox 0bp 0bp 432bp 280bp

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Comparison of the key overlap ratio, in fingerprint task performed on sketches
 of CQT representation performed with PP and IHT sparses algorithmes.
 The figures show the results on signal reconstructed with wihte noise,
 in rigth the SNR is -5DB and in left the SNR is 3DB.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/NoiseRobustness_k200.pdf
	scale 50

\end_inset


\begin_inset Graphics
	filename figures/NoiseRobustness_k20.pdf
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Comparison of the key overlap ratio, in fingerprint task performed on sketches
 of CQT representation performed with PP and IHT sparses algorithmes, with
 a fixe number of features and over a bench of diffrents SNR in signal reconstru
ction.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph*
Conclusion: The results on IHT/PP algorithm are different that what we observed
 in the second experiment with listeners:
\end_layout

\begin_layout Itemize
The IHT algorithm is far more robust to noise than PP.
 The recognition performance of the PP algorithm is better than IHT only
 if the signal is clear (3DB noise) and if the set of sparse features is
 small (20 features).
 In the case of a small number of sparse feature the algorithm IHT peak
 to closed features.
\end_layout

\begin_layout Itemize
The fingerprint process is a highly distorted proxy to the processing of
 human listener for the task of recognizing sounds.
 Listener performed better understanding on PP sparse features for reconstructio
n.
 
\end_layout

\begin_layout Itemize
The first experience raised unanswered questions.
 Is the understanding of sketches related to the time of the signal excerpt?
 if not, how can we explain the decreasing result on experiment 1? Are the
 listening experiences similar in CQT representation than in Cochleograms?
\end_layout

\begin_layout Chapter*
Conclusions and future works
\end_layout

\begin_layout Section*
Achievements
\end_layout

\begin_layout Subsubsection*
Generalized Fingerprinting framework
\end_layout

\begin_layout Standard
The Audio fingerprint problem can be cast as a set of constraints on the
 discriminative power versus robustness of the selected keypoints.
 With proper formulation, we have found that existing methods can be seen
 as particular cases of a larger family.
\end_layout

\begin_layout Subsubsection*
Trying new sketches
\end_layout

\begin_layout Standard
Our experiments show that Shifting from auditory-based representations to,
 for instance, CQT is a good idea since it does decrease computation loads
 while slightly improving recognition performances.
\end_layout

\begin_layout Subsubsection*
Functional comparison framework
\end_layout

\begin_layout Standard
For this project, we have implemented more than 20 different types of sketches,
 each of which has many parameters.
 Among them we have been able to discriminate the ones that showed promising
 recognition results and optimize their parameters.
 
\end_layout

\begin_layout Subsubsection*
Measure the gap between psycho-physical results and computer-based approach.
\end_layout

\begin_layout Standard
The laboratory experiment we did on IHT and PP algorithm shows that the
 fingerprint recognition is different to listeners one.
 Also the SineWave Speech seem to be very hard to include in a fingerprinting
 framework.
\end_layout

\begin_layout Section*
Under exploration
\end_layout

\begin_layout Subsubsection*
Using MPCA as sketches
\end_layout

\begin_layout Standard
The MPCA can perform a good basis of sketches, but the input (and output)
 as to be real (problem) of the inverse construction.
\end_layout

\begin_layout Subsubsection*
Finding higher-dimensional features for speech recognition
\end_layout

\begin_layout Standard
Although their use is problematic in terms of computational load.
 Cortico and Quorticograms remains a very promising field for discovering
 structure that capture high-level information.
\end_layout

\begin_layout Subsubsection*
Others
\end_layout

\begin_layout Itemize
Intermediate work on vowels databases,: 1: to compare the listeners recognition
 of CQT over Cochleograms.
 2: to make a step in the analysis of the variability of Corticograms, before
 speech database.
\end_layout

\begin_layout Itemize
Works on time alignment algorithm.
\end_layout

\begin_layout Itemize
Work on MPCA, trying to find a way to use input complex data, and having
 the possibility of working with 4 dimensional objects.
\end_layout

\begin_layout Itemize
Works on dictionary and find a theoretic link between MPCA and sparse recommenda
tion: MPCA can be reformulate as finding an orthonormal basis (dictionary),
 highlighting the variability, in the least square sense.
 
\end_layout

\begin_layout Section*
Link with next WP
\end_layout

\begin_layout Standard
As the second phase of the project is starting we may draw some more general
 conclusions.
 Even though we were not able to match its properties to our metric system,
 Sine Wave Speech should be considered as a primary sketchification method.
 According to our results, we think that sparse representation using the
 CQT and a IHT algorithm should also be tested on human listeners as well
 as other sketches build from the middle scale/rate dimensions of the Quorticogr
am.
\end_layout

\begin_layout Standard
The psycho-acoustic experiments to come will certainly raise many more questions
 and require that the developed framework be tested on new problems.
 At the end of WP1, we can confidently say that the framework is ready.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "library"
options "bibtotoc,abbrv"

\end_inset


\end_layout

\end_body
\end_document
